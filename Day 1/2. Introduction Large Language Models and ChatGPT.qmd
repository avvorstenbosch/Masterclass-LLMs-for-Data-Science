---
title: "Introduction to Large Language Models and ChatGPT" 
subtitle: ""
author: "Alex van Vorstenbosch"
footer: "LLMs and ChatGPT"
title-slide-attributes:
  data-background-image: "./figures/LLMs-and-chatgpt.png"
  data-background-opacity: "0.5"
  data-background-size: cover
institute: "Erasmus Q-intelligence "
date: "11-17-2023"
format:
    revealjs:
        auto-fit: true
        width: 1600
        height: 1000
        slide-number: true
        chalkboard: 
            buttons: false
        preview-links: true 
        theme: night 
        scrollable: false
        css: style.scss
execute:
    warning: false
    error: false
---

# What is ChatGPT?
:::: {.columns}

::: {.column width="60%"}
- A Large Language model trained on enormous amounts of data:
    - Which was instruction tuned
    - Which was finetuned with RLHF
- Which resulted in:
    - *Extremely* impressive Chatbot capabilities
    - *Much* better interaction and
    - *Much* better language understanding while also 
    - *Accessible* to use for **Anybody**
:::
::: {.column width="40%"}
<iframe src="https://chat.openai.com/share/7c278d44-48bd-47c7-8523-ce080b4d4ee1" class="iframe-chatgpt"></iframe>
:::
:::

## Overview
- A brief history of LLMs
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

## Overview
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

# A brief history of LLMs
## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

A token: *a single character, a combination of characters, or a word*

## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

This is nothing new, your phone does something similair: 

![](./figures/text-prediction-whatsapp.png){fig-align="center"}

## {.full-bg-slide background-image="./figures/slide-chatpgtinternals.png"}

## Byte-Pair Encoding Tokenizer
- Easy algorithm to *compress* text into most common elements
- [Flying, Trying, Sky, Cry, Sly] --> "F, L, Y, I, N, G, T, R, K, C, S"
- "F, L, Y, I, N, G, T, R, K, C, S, **IN**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, **LY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, **RY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, RY, **ING**"
- etc...

## Byte-Pair Encoding Tokenizer
- Able to:
    - encode you whole vocabulary per definition
    - Chose precize size you want for your model
    - Assign tokens to most *important* parts of vocabulary
- This does mean that English gets more tokens than Dutch:

:::{.columns}
:::{.column width="50%"}
![](./figures/tokenizer-dutch.png)
:::

:::{.column width="50%"}
![](./figures/tokenizer-english.png)

:::
:::

- [Try it Yourself](https://platform.openai.com/tokenizer)

## One-hot Encoding {.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
1\\
0\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
1\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
0\\
1
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Sparse vector of the vocabulary dimension
- 3 out of 4 numbers are uninformative
- 'Expensive' for large corpus of text
- Can we do better?
:::

## Word embeddings{.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Dense vectors of Dimension N (hyperparameter of model ~ 728)
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- Meaningfull representation
- Encoded semantic information: 
:::
::::{.inline-fragment style="margin-left: 25%; text-align: center"}
:::{.fragment} 
King - Man + Woman = 
:::
:::{.fragment} 
Queen 
:::
::::
:::{.incremental}
- These embeddings marked the start of the new NLP era ^[*2013 - Efficient Estimation of Word Representations in
Vector Space[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)*] 
- This still has some issues...
:::

## Word embeddings{.smaller}

$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$

- Dense vectors
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- The numbers are now informative to qualities of the token
- Semantiscly-meaningfull: 

:::{style="text-align: center"}
King - Man + Woman = Queen 
:::
 
- This still causes problems...

## Transformer Embeddings {.smaller}
:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.75
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.54\\
-0.79\\
-0.34\\
\phantom{-}0.22
\end{pmatrix} & \begin{pmatrix}
-0.41\\
\phantom{-}0.79\\
\phantom{-}0.17\\
\phantom{-}0.84
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Word-embedding now depends on context
- Able to encode even more meaningfull information
- Emperically this just works!
- The start of the new age of NLP ^[*2017 - Attention is all you need [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*]
:::

## What Does self-attention look like {auto-animate="true"}
![](figures/fully_connected.png){fig-align="center"}

## What Does self-attention look like {auto-animate="true"}

![](figures/fully_connected.png){.absolute left="70%" top="70%" width="auto" height="250px"}
$$
Attention \sim Query \cdot Key^{T} 
$$

:::{.incremental}
- Conceptual Interpretation:
    - Query: I have a *Noun*, I need a Subject!
    - Key: I have a *Subject* here. 
:::

## What does self-attention look like

<iframe width="80%" height="80%" justify-content=center src="./figures/html_head.html" style="display: block; margin: 0 auto;"></iframe>


::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Are you still following what is happening? {auto-animate="true"}

<iframe width="80%" height="80%" src="./figures/html_transformer.html" scrollable="no"style="display: block; margin: 0 auto;"></iframe>

::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Attention and BERT
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- Pretraining on massive corpora of text
- Very powerfull contextual embeddings of language
- *State-of-the-art* on many language tasks with finetuning
    - Sentiment Analysis
    - Text Classification
    - Named entity recognition
    - Question Answering
    - Language Modeling
- Bert-base: 110m parameters
- Bert-large: 340m parameters

## If we have more compute ...
- What if we want to improve our models.
- Companies like OpenAI have more compute available, what should they do?

:::{.columns}

:::{.column width="50%"}
![](./figures/openai-compute-curves.png)
Optimal model size grows smoothly with the loss target and compute $\mathrm{budget^1}$
:::

:::{.column width="50%"}
![](./figures/openai-compute-allocation.png)
For optimally compute-efficient training, most of the increase should go towards increased model
size^[[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)]
:::
:::

## ... we puth it towards the model scale
![Alt text](./figures/model_size_growing.jpg)
Models just kept on growing, credit: [Julien Simon, Huggingface](https://huggingface.co/blog/large-language-models)

## GPT3 *IS* ChatGPT, almost ...
- [2020 - Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- These models are so good at language modeling (*SOTA*), that finetuning is no longer needed to perform NLP task
- Very strong 0-shot performance in many NLP task (already in GPT2)
- Can perform **In-context learning**:
    - Given a few examples, learn how to perform task
    - *No model parameters are adjusted at any point*

## 
![[Andrej Karpathy, Microsoft - State of GPT](https://karpathy.ai/stateofgpt.pdf)](./figures/stateofgpt.png)

## Instruction tuning (finetuning)

- The models just predict likely continuations of text.
- This does not match with the behaviour we seek

:::{.panel-tabset}
## Base model
```
INPUT:
Explain what a Large Language Model is.

OUTPUT:
Explain what a transformer model is.

Explain what a tokenizer is.

Explain what gradient descent is.
```

## Instruction tuning 
```
INPUT:
Explain what a Large Language Model is.

HUMAN EXAMPLE:
A Large Language Model is a foundational language model with typically billions of parameters. These models have become popular in recent years because of their ease of use combined with impressive performance across the board on NLP-tasks
```
:::


## A brief history of NLP
- Classic ML: Bag-of-Words, TF-IDF etc.
- Word2Vec - Neural Informative Representations
- Transformers - Context based representations
- GPT2 Keep scaling this up
- Scaling laws --> Large language models i.e. GTP3
- `emergent` properties
- Instruction-tuning and RLHF --> ChatGPT

## Overview
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

## Overview
- A brief history of LLMs
- <span class="highlighted-text">Capabilities of ChatGPT</span>
- Real-world applications of ChatGPT

# Capabilities of ChatGPT
## But very powerfull with many strong properties:
- Understand language conventions (syntax, grammer, etc.)
- Can answer questions (Have internal knowledge, )
- Can code
- Can write just like humans, and in specific styles
- Can do some basic arithmatic
- Can understand sentiment, style, etc.
- Can do logical reasoning (too some extent)

## How does a LLM work
- Training:
    - Pretraining
    - Finetuning
    - RLHF

- Processing data:
    - BPE-tokenizer
    - Embeddings
    - (Self-)Attention Mechanism 
    - output softmax predictions (with temperature)

## Strenghts of LLMs

## Weaknesses of LLMs

## How you should not think of ChatGPT

## Overview
- A brief history of LLMs
- <span class="highlighted-text">Capabilities of ChatGPT</span>
- Real-world applications of ChatGPT

## Overview
- A brief history of LLMs
- Capabilities of ChatGPT
- <span class="highlighted-text">Real-world applications of ChatGPT </span>

# Real world applications of ChatGPT