---
title: "Introduction to Large Language Models and ChatGPT" 
subtitle: ""
author: "Alex van Vorstenbosch"
footer: "LLMs and ChatGPT"
title-slide-attributes:
    data-background-image: "./figures/LLMs-and-chatgpt.png"
    data-background-opacity: "0.5"

date: "11-17-2023"
---

# What is ChatGPT?
:::: {.columns}

::: {.column width="60%"}
- A Large Language model trained on enormous amounts of data:
    - Which was instruction tuned
    - Which was finetuned with RLHF
- Which resulted in:
    - *Extremely* impressive Chatbot capabilities
    - *Much* better interaction and
    - *Much* better language understanding while also 
    - *Accessible* to use for **Anybody**
:::
::: {.column width="40%"}
<iframe src="https://chat.openai.com/share/7c278d44-48bd-47c7-8523-ce080b4d4ee1" class="iframe-chatgpt"></iframe>
:::
:::

## Overview
- A brief history of LLMs
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

## Overview
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

# A brief history of LLMs
## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

A token: *a single character, a combination of characters, or a word*

## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

This is nothing new, your phone does something similair: 

![](./figures/text-prediction-whatsapp.png){fig-align="center"}

## {.full-bg-slide background-image="./figures/slide-chatpgtinternals.png"}

## Byte-Pair Encoding Tokenizer
- Easy algorithm to *compress* text into most common elements
- [Flying, Trying, Sky, Cry, Sly] --> "F, L, Y, I, N, G, T, R, K, C, S"
- "F, L, Y, I, N, G, T, R, K, C, S, **IN**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, **LY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, **RY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, RY, **ING**"
- etc...

## Byte-Pair Encoding Tokenizer
- Able to:
    - encode you whole vocabulary per definition
    - Chose precize size you want for your model
    - Assign tokens to most *important* parts of vocabulary
- This does mean that English gets more tokens than Dutch:

:::{.columns}
:::{.column width="50%"}
![](./figures/tokenizer-dutch.png)
:::

:::{.column width="50%"}
![](./figures/tokenizer-english.png)

:::
:::

- [Try it Yourself](https://platform.openai.com/tokenizer)

## One-hot Encoding {.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
1\\
0\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
1\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
0\\
1
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Sparse vector of the vocabulary dimension
- 3 out of 4 numbers are uninformative
- 'Expensive' for large corpus of text
- Can we do better?
:::

## Word embeddings{.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Dense vectors of Dimension N (hyperparameter of model ~ 728)
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- Meaningfull representation
- Encoded semantic information: 
:::
::::{.inline-fragment style="margin-left: 25%; text-align: center"}
:::{.fragment} 
King - Man + Woman = 
:::
:::{.fragment} 
Queen 
:::
::::
:::{.incremental}
- These embeddings marked the start of the new NLP era ^[*2013 - Efficient Estimation of Word Representations in
Vector Space[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)*] 
- This still has some issues...
:::

## Word embeddings{.smaller}

$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$

- Dense vectors
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- The numbers are now informative to qualities of the token
- Semantiscly-meaningfull: 

:::{style="text-align: center"}
King - Man + Woman = Queen 
:::
 
- This still causes problems...

## Transformer Embeddings {.smaller}
:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.75
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.54\\
-0.79\\
-0.34\\
\phantom{-}0.22
\end{pmatrix} & \begin{pmatrix}
-0.41\\
\phantom{-}0.79\\
\phantom{-}0.17\\
\phantom{-}0.84
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Word-embedding now depends on context
- Able to encode even more meaningfull information
- Emperically this just works!
- The start of the new age of NLP ^[*2017 - Attention is all you need [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*]
:::

## What Does self-attention look like {auto-animate="true"}
![](figures/fully_connected.png){fig-align="center"}

## What Does self-attention look like {auto-animate="true"}

![](figures/fully_connected.png){.absolute left="70%" top="70%" width="auto" height="250px"}
$$
Attention \sim Query \cdot Key^{T} 
$$

:::{.incremental}
- Conceptual Interpretation:
    - Query: I have a *Noun*, I need a Subject!
    - Key: I have a *Subject* here. 
:::

## What does self-attention look like

<iframe width="80%" height="80%" justify-content=center src="./figures/html_head.html" style="display: block; margin: 0 auto;"></iframe>


::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Are you still following what is happening? {auto-animate="true"}

<iframe width="80%" height="80%" src="./figures/html_transformer.html" scrollable="no"style="display: block; margin: 0 auto;"></iframe>

::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Sampling output tokens
- Output is sampled, and therefore a stochastic:
    - **Running the same prompt twice will give 2 different results**
$$
P_i = \frac{e^{\frac{y_i}{T}}}{\sum_i^n e^{\frac{y_i}{T}}} = \frac{(e^{y_i})^\frac{1}{T}}{\sum_i^n (e^{y_i})^\frac{1}{T}}
$$

:::{.panel-tabset}

## T = 1 (normal)

![](./figures/ptoken-t1.png){fig-aling=center}

## T = 0.5 

![](./figures/ptoken-t05.png){fig-aling=center}


## T = 2

![](./figures/ptoken-t2.png){fig-aling=center}


## T = 0

![](./figures/ptoken-t0.png){fig-aling=center}

:::


## Attention and BERT
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- Pretraining on massive corpora of text
- Very powerfull contextual embeddings of language
- *State-of-the-art* on many language tasks with finetuning:
    - Sentiment Analysis
    - Text Classification
    - Named entity recognition
    - Question Answering
    - Language Modeling
- Bert-base: 110m parameters
- Bert-large: 340m parameters

## If we have more compute ...
- What if we want to improve our models.
- Companies like OpenAI have more compute available, what should they do?

:::{.columns}

:::{.column width="50%"}
![](./figures/openai-compute-curves.png)
Optimal model size grows smoothly with the loss target and compute $\mathrm{budget^1}$
:::

:::{.column width="50%"}
![](./figures/openai-compute-allocation.png)
For optimally compute-efficient training, most of the increase should go towards increased model
size^[[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)]
:::
:::

## ... we puth it towards the model scale
![Alt text](./figures/model_size_growing.jpg)
Models just kept on growing, credit: [Julien Simon, Huggingface](https://huggingface.co/blog/large-language-models)

## GPT3 *IS* ChatGPT, almost ...
- [2020 - Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- These models are so good at language modeling (*SOTA*), that finetuning is no longer needed to perform NLP task
- Very strong 0-shot performance in many NLP task (already in GPT2)
- Can perform **In-context learning**:
    - Given a few examples, learn how to perform task
    - *No model parameters are adjusted at any point*
    - `Emergent property` of LLMs

## 
![[Andrej Karpathy, Microsoft - State of GPT](https://karpathy.ai/stateofgpt.pdf)](./figures/stateofgpt.png)

## Instruction tuning (finetuning)

- The models just predict likely continuations of text.
- This does not match with the behaviour we seek

:::{.panel-tabset}
## Base model
```
INPUT:
Explain what a Large Language Model is.

OUTPUT:
Explain what a transformer model is.

Explain what a tokenizer is.

Explain what gradient descent is.
```

## Instruction tuning 
```
INPUT:
Explain what a Large Language Model is.

HUMAN EXAMPLE:
A Large Language Model is a foundational language model with typically billions of parameters. These models have become popular in recent years because of their ease of use combined with impressive performance across the board on NLP-tasks
```
:::

## Reinforcement Learning From Human Feedback

- Not just showing example completions, but also rating them.
- Apply Reward Modeling and Reinforcement learning to 
tune the model towards higher quality responses.
- Very difficult to train, but...
- It just works better.

## And this gives you...

<iframe src="https://chat.openai.com/share/36173d8f-2a3e-4994-8a6a-04cc5188d97f" class="iframe-chatgpt"></iframe>

## Overview
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
- Real-world applications of ChatGPT

## Overview
- A brief history of LLMs
- <span class="highlighted-text">Capabilities of ChatGPT</span>
- Real-world applications of ChatGPT

# Capabilities of ChatGPT

## Strenghts of LLMs

Foundational language model that:

- Understands language conventions (syntax, grammer, etc.)
- can answer questions (Has internal knowledge base)
- can code
- Can write just like humans
- Can do some basic arithmatic
- Can understand *deep* elements of writing: sentiment, style, etc.
- Can do logical reasoning (to some extent)
- [Capabilities measured on easy scales](https://openai.com/research/gpt-4)

## Weaknesses of LLMs

- Limited input window (4096 tokens)
- It has no memory
- It can only predict next tokens, nothing else:
    - It cannot perform tasks while you wait!
- It cannot think in multiple directions like we do!
- It doesn't understand it's own internal model!
- It doesn't know truth and what is not

## How you should not think of ChatGPT

- We tend to feel empathy for ChatGPT
    - People like to say thank you to ChatGPT
    - It feels very `human`
- BUT: it really boils down to a very very big whatsapp next-word predictor
- WE CAN BREAK THE MODEL LOGIC

## Repeated sampling Penalty
- Generative language models have a tendency to repeat themselves:
    - Therefore the sampling algorithm receives a `repetition penalty`
    - This can be exploited

<iframe src="https://chat.openai.com/share/cfe864c6-4213-4be5-b70c-f66a8c55a3fd" class="iframe-chatgpt" style="width: 700px; height: 900px;" ></iframe>

## Glitch Tokens
- Some tokens identified by BPE only appear in `useless` context
    - Such as /r/counting
    - usernames get very high frequency and get their own tokens

:::{.columns}

:::{.column width="40%"}

<iframe src="https://chat.openai.com/share/f4a8c854-e3ff-4032-8683-40b31f1444e2" class="iframe-chatgpt"></iframe>

:::

:::{.column width="60%"}

<iframe width="768px" height="480px" src="https://www.youtube.com/embed/WO2X3oZEJOA?si=Cz6-j5f_Yvr4zvyC&amp;clip=UgkxQwEYjoJKAiZBm9KEcOIswK1XsixP9nJ0&amp;clipt=EAAY758C" frameborder="12px" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture;" allowfullscreen></iframe>

:::

:::


## Overview
- A brief history of LLMs
- <span class="highlighted-text">Capabilities of ChatGPT</span>
- Real-world applications of ChatGPT

## Overview
- A brief history of LLMs
- Capabilities of ChatGPT
- <span class="highlighted-text">Real-world applications of ChatGPT </span>

# Real world applications of ChatGPT

- Pair Programming
- Language Modeling Tasks
- Text Generation
- Inspiration
- Explanation