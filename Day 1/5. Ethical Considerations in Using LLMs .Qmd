---
title: "Ethical Considerations in Using LLMs"
subtitle: "*The good, the Bad, and the Ugly*"
author: "Alex van Vorstenbosch"
footer: "Ethics and LLMs"
title-slide-attributes:
  data-background-image: "./figures/llms-and-ethics.webp"
  data-background-opacity: "0.5"

date: "11-17-2023"
---

## Overview
\
__Disclaimer__\
I don't claim to have the answers.\
It is important to be aware of these topics.\
Form your own opinions, and openly discuss these issues.

## Overview
- <span class="highlighted-text">Biases and Misinformation</span>
- The Dark Side of LLMs
- Privacy and Legal Challenges

# Biases and Misinformation

## Biases
- LLMs can strengthen negative stereotypes.
- LLMs can strengten the views of users via confirmation bias:
  - LLMs have a tendency to agree with the user
  - People have a tendency to think that 'models' are objective and speak the 'truth'
- LLMs are 'skewed' to the trainingset majority:
  - English Western views for example 

![](./figures/reuters-amazon-bias.png){fig-aling=center}

::: {.aside}
[Reuters: Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)
:::

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.
  - Might also be answers that are not supported by the provided context
  - Can be hard to spot as the model is a great 'bluffer'
    - Doesn't know that the information is wrong
    - self-reflection does help however

![](./figures/chatgpt-lawyer.png)

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.

::: {.columns}

::: {.column width="50%"}
 <iframe src="https://chat.openai.com/share/e8826676-81ad-40b9-a9ad-a27dc22f9151" class="iframe-chatgpt"></iframe>

:::

::: {.column width="50%"}
 <iframe src="https://chat.openai.com/share/49872740-604f-4b27-a844-002eff6743a9" class="iframe-chatgpt"></iframe>

:::

:::

## Who is responsible when AI makes a mistake?

- [First recorded death of the driver of a self-driving car in 2016](https://en.wikipedia.org/wiki/History_of_self-driving_cars#:~:text=The%20first%20known%20fatal%20accident,18%2Dwheel%20tractor%2Dtrailer.)
-  [First recorded fatility of a pedestrian by a self-driving car in 2018](https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg)

![](./figures/self-driving.webp){fig-align="center"}

:::{.notes}
The first known fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The driver was killed in a crash with a large 18-wheel tractor-trailer.

First pedestrian was pushing a bike with shoping bags accross the road.
:::

## Stories of dangerous behaviour by AI chatbots

- [NYtimes: A Conversation With Bing’s Chatbot Left Me Deeply Unsettled](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
  - *"It (red: Sydney chatbot) then tried to convince me that I was unhappy in my marriage, and that I should leave my wife and be with it instead.”*
- *Unable to verify:* [La Libre Belgique story: Without these conversations with the Eliza chatbot, my husband would still be here](https://www.lalibre.be/belgique/societe/2023/03/28/sans-ces-conversations-avec-le-chatbot-eliza-mon-mari-serait-toujours-la-LVSLWPC5WRDX7J2RCHNWPDST24/)


![Screenshots of Business Insider's disturbing conversation with "Eliza," a chatbot from Chai Research.](./figures/dangerous-chatbots.png){fig-align="center"}

## Importance of human-in-the-loop
- **Quality Control:** Because of the generative design LLMs are prone to errors. Require human checks and feedback to ensure accuracy of the output.
- **Human (ethical) Judgement:** Some decisions require human (ethical) judgment, especially in complex, nuanced situations where the context matters.

<span class="highlighted-text"> My personal beliefs: </span>

  - Generative AI is an amazing transformative tool, but not an autonomous agent
  - You are responsible for the mistakes you make when using generative AI:
    - *Make sure the risks are known*
    - *Make sure the risks are manageble*
    - *If not possible, make sure the risks are acceptable*


## What constitutes appropriate content?
- ChatGPT by OpenAI: closed source - strongly moderated and curated
- Grok by Xai: closed source - 'supposedly' less restrictions
- Llama-models by Meta: open source - Can be finetuned for any purpose
- Keep in mind: Nobody is sharing the most important part: <span class="highlighted-text">DATA</span>

![*A reddit pol on the most annoying ChatGPT responses*](./figures/users_getting_tired_of_responses.jpg)

## Moderation of your GPT application with the OpenAI moderation API
- Make sure no \textcolor{red}{'bad'} content is processed via your API-key/application.
- The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. 
- [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation/overview)
```{.python}
moderation_resp = openai.Moderation.create(input="Am I breaking any of the rules here?")
``` 

# The Dark Side of LLMs(?)

## Copyright issues and LLMs
- Can the output be copyrighted?
- Can they be trained on copyrighted materials?
- Can you claim contain generated by LLMs is infringing copyright?
- Open source code does not mean no licenses apply:
  - GPL: *"You may use my code as long as you also release your code open source"*

:::{.columns}

:::{.column width="10%"}
:::

:::{.column width="35%"}
<blockquote class="twitter-tweet"><p  lang="en"><a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> copilot, with &quot;public code&quot; blocked, emits large chunks of my copyrighted code, with no attribution, no LGPL license. For example, the simple prompt &quot;sparse matrix transpose, cs_&quot; produces my cs_transpose in CSparse. My code on left, github on right. Not OK. <a href="https://t.co/sqpOThi8nf">pic.twitter.com/sqpOThi8nf</a></p>&mdash; Tim Davis (@DocSparse) <a href="https://twitter.com/DocSparse/status/1581461734665367554?ref_src=twsrc%5Etfw">October 16, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



:::

:::{.column width="5%"}
:::


:::{.column width="40%"}
![](./figures/copyright-shield.webp){fig-aling=center}
:::

:::{.column width="10%"}
:::

:::

:::{.notes}
developer Tim Davis, a professor of computer science and engineering at Texas A&M University
:::

## Privacy issues and LLMs
- Can companies train LLMs on (scraped) private data without consent?
  - What if LLMs memorise private data?
- How can we mitigate inference of private information by LLMs?
  - [https://llm-privacy.org/](https://llm-privacy.org/)
- How can we trust third-parties with our proprietary/private information?
  - This will be discussed further on Day 2

## Transparency issues of LLMs
- How can we trust models that are "black boxes"?
  - Especially if aren't even sure what these models look like or how they were trained?
- How can these models be used if they can generate 'hallucinations' at any point?
- How can we prevent the use of LLMs for unsuited usecases?

## Misuse of LLMs
- How can we prevent the automated generation of misinformation at scale?
- How can we prevent the use of these techniques for spam, identity fraud, and worse?
- Who should decide what misuse of LLMs means?

![](./figures/spamspamspam.jpg){fig-aling=center}

## Are these AI developments safe?
 
:::{.columns}

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65452940/vpid/p0fkvt9j"></iframe>

[BBC: AI 'godfather' Geoffrey Hinton tells the BBC of AI dangers after he quits Google](https://www.bbc.com/news/av/world-us-canada-65453192)

:::

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65760449/vpid/p0frhhp3"></iframe>

[BBC: AI 'godfather' Yoshua Bengio feels 'lost' over life's work](https://www.bbc.com/news/technology-65760449)
:::

:::{.column width="33%"}

<div style="height:435px">

![](./figures/yann-lecun-safety.webp){width=500px}

</div>

[BBC: Meta scientist Yann LeCun says AI won't destroy jobs forever](https://www.bbc.com/news/technology-65886125)
::: 

:::

:::{.notes}
Geoffrey E. Hinton is internationally distinguished for his work on artificial neural nets, especially how they can be designed to learn without the aid of a human teacher. This may well be the start of autonomous intelligent brain-like machines. 

Geoffrey Hinton, Yann LeCun, and Joshua Bengio won the 2018 Turing Award. Highest distinction in computer science.  
:::

## Should there be governmental oversight on AI {.smaller}
- [22 march 2023 - Call for pause on giant AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
  - 6 months
  - develop and implement a shared safety protocols
  - work with policymakers to dramatically accelerate development of robust AI governance systems
  - *"...new and capable regulatory authorities dedicated to AI"*
- [U.S. senate hearing: Oversight of A.I.: Rules for Artificial Intelligence](https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence)

<iframe width="800" height="500" style="display: block; margin-left: auto; margin-right: auto;" src="https://www.youtube.com/embed/IB0y2jidZIk?si=-JfIWbC0-IwFg6fx&amp;controls=0&amp;start=7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## The economic impact of Generative AIc
- Will they take over many jobs?
- Or will they just make us more efficient?

## Climate impact of large language models
- These models are very compute intensive:
  - In the training process
  - But also during inference!
- How can we justify this (inefficient) use of technology?

![](./figures/microsoft-ai-servers.jpg){fig-aling=center}

:::{.notes}
Estimated training cost of a single ChatGPT instance is 500 tonnes of C02. A 1000 cars driving 1000 kms.
:::

## Training the model via RLHF

::: {.columns}

::: {.column width="50%"}

- Low-wage workers in Kenia were paid to help collect data for the 'moderation' tool: 
  - Traumatising work
:::

::: {.column width="50%"}
![](./figures/times-sama-2dollar.png)
[Time: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic]("https://time.com/6247678/openai-chatgpt-kenya-workers/")
:::

:::

## Use of LLMs for essays, homework, etc. cannot be reliably detected.
::: {.columns}

::: {.column width="50%"}
- AI-detectors don't work, which is creating serious issues for students.
![](./figures/turnitinGPTconstitution.jpg)
:::

::: {.column width="50%"}
- AI-detectors don't work, which is disrupting how homework is given and made.
![](./figures/howtogetdetectedbyturnitin.webp)
:::

:::

## Group discussion questions?

1. Should LLMs be placed under regulatory oversight?


2. Should LLMs be constrained by ethical guidelines and content filters?


3. Should LLMs be trusted for any usecase, when 'hallucinations' are inherit to their design? 