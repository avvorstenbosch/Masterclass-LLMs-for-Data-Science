---
title: "Ethical Considerations in Using LLMs"
subtitle: "*The good, the Bad, and the Ugly*"
author: "Alex van Vorstenbosch"
footer: "Ethics and LLMs"
title-slide-attributes:
  data-background-image: "./figures/llms-and-ethics.webp"
  data-background-opacity: "0.5"

date: "11-17-2023"
---

## Overview
\
__Disclaimer__\
I don't claim to have the answers.\
It is important to be aware of these topics.\
Form your own opinions, and openly discuss these issues.

## Overview
- <span class="highlighted-text">Biases and Misinformation</span>
- The Dark Side of LLMs
- Privacy and Legal Challenges

# Biases and Misinformation

## Biases
- LLMs can strengthen negative stereotypes.
- LLMs can strengten the views of users via confirmation bias:
  - LLMs have a tendency to agree with the user
  - People have a tendency to think that 'models' are objective and speak the 'truth'
- LLMs are 'skewed' to the trainingset majority:
  - English Western views for example 

![](./figures/reuters-amazon-bias.png){fig-aling=center}

::: {.aside}
[Reuters: Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)
:::

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.
  - Might also be answers that are not supported by the provided context
  - Can be hard to spot as the model is a great 'bluffer'
    - Doesn't know that the information is wrong
    - self-reflection does help however

![](./figures/chatgpt-lawyer.png)

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.

::: {.columns}

::: {.column width="50%"}
 <iframe src="https://chat.openai.com/share/e8826676-81ad-40b9-a9ad-a27dc22f9151" class="iframe-chatgpt"></iframe>

:::

::: {.column width="50%"}
 <iframe src="https://chat.openai.com/share/49872740-604f-4b27-a844-002eff6743a9" class="iframe-chatgpt"></iframe>

:::

:::

## Who is responsible when AI makes a mistake?

- [First recorded death of driver of self-driving car](https://en.wikipedia.org/wiki/History_of_self-driving_cars#:~:text=The%20first%20known%20fatal%20accident,18%2Dwheel%20tractor%2Dtrailer.)
-  [First recorded fatility of pedestrian by self-driving car](https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg)

:::{.notes}
The first known fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The driver was killed in a crash with a large 18-wheel tractor-trailer.

First pedestrian was pushing a bike with shoping bags accross the road.
:::

# The Dark Side of LLMs(?)

## Moderation of your GPT application with the OpenAI moderation API
- Make sure no `bad` content is processed via your API-key
- The moderation endpoint is free to use when monitoring the inputs and outputs of OpenAI APIs. 
- [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation/overview)
```{.python}
moderation_resp = openai.Moderation.create(input="Am I breaking any of the rules here?")
``` 
## Copyright issues and LLMs
- Can the output be copyrighted?
- Can they be trained on copyrighted materials?
- Can you claim contain generated by LLMs is infringing copyright?

![](./figures/copyright-shield.webp){fig-aling=center}

## Privacy issues and LLMs
- Can companies train LLMs on (scraped) private data without consent?
  - What if LLMs memorise private data?
- How can we mitigate inference of private information by LLMs?
  - [https://llm-privacy.org/](https://llm-privacy.org/)
- How can we trust third-parties with our proprietary/private information?
  - This will be discussed further on Day 2

## Transparency issues of LLMs
- How can we trust models that are "black boxes"?
  - Especially if aren't even sure what these models look like or how they were trained?
- How can these models be used if they can generate 'hallucinations' at any point?
- How can we prevent the use of LLMs for unsuited usecases?

## Misuse of LLMs
- How can we prevent the automated generation of misinformation at scale?
- How can we prevent the use of these techniques for spam, identity fraud, and worse?
- Who should decide what misuse of LLMs means?

![](./figures/spamspamspam.jpg){fig-aling=center}

## Are these AI developments safe?
 
:::{.columns}

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65452940/vpid/p0fkvt9j"></iframe>

[BBC: AI 'godfather' Geoffrey Hinton tells the BBC of AI dangers after he quits Google](https://www.bbc.com/news/av/world-us-canada-65453192)

:::

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65760449/vpid/p0frhhp3"></iframe>

[BBC: AI 'godfather' Yoshua Bengio feels 'lost' over life's work](https://www.bbc.com/news/technology-65760449)
:::

:::{.column width="33%"}

<div style="height:435px">

![](./figures/yann-lecun-safety.webp){width=500px}

</div>

[BBC: Meta scientist Yann LeCun says AI won't destroy jobs forever](https://www.bbc.com/news/technology-65886125)
::: 

:::

:::{.notes}
Geoffrey E. Hinton is internationally distinguished for his work on artificial neural nets, especially how they can be designed to learn without the aid of a human teacher. This may well be the start of autonomous intelligent brain-like machines. 

Geoffrey Hinton, Yann LeCun, and Joshua Bengio won the 2018 Turing Award. Highest distinction in computer science.  
:::

## The economic impact of Generative AIc
- Will they take over many jobs?
- Or will they just make us more efficient?

## Climate impact of large language models
- These models are very compute intensive:
  - In the training process
  - But also during inference!
- How can we justify this (inefficient) use of technology?

![](./figures/microsoft-ai-servers.jpg){fig-aling=center}

:::{.notes}
Estimated training cost of a single ChatGPT instance is 500 tonnes of C02. A 1000 cars driving 1000 kms.
:::

## Training the model via RLHF

::: {.columns}

::: {.column width="50%"}

- Low-wage workers in Kenia were paid to help collect data for the 'moderation' tool: 
  - Traumatising work
:::

::: {.column width="50%"}
![](./figures/times-sama-2dollar.png)
[Time: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic]("https://time.com/6247678/openai-chatgpt-kenya-workers/")
:::

:::

## Use of LLMs for essays, homework, etc. cannot be reliably detected.
::: {.columns}

::: {.column width="50%"}
- AI-detectors don't work, which is creating serious issues for students.
![](./figures/turnitinGPTconstitution.jpg)
:::

::: {.column width="50%"}
- AI-detectors don't work, which is disrupting how homework is given and made.
![](./figures/howtogetdetectedbyturnitin.webp)
:::

:::

## Propositions