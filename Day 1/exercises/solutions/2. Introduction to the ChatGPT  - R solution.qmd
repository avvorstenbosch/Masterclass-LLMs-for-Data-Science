---
title: "Introduction to the ChatGPT API" 
date: "11-17-2023"
format: 
    html:
        number-sections: true
        page-layout: full
        title-block-banner: true
        self-contained: true
include-in-header:
  - text: |
      <style>
      .cell-output-stdout code {
        word-break: break-wor !important;
        white-space: pre-wrap !important;
      }
      </style>
---

# SETUP: OPENAI_API_KEY
We'll start by setting up your environment for working with the OPENAI-API.
You can generate your API key at [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys). 

1. Generate your key and copy it, \textcolor{red}{you cannot view it again!}

2. Save the key in a .yml file, a .env file, or your Renviron (Which you can find under your home dir). A .yml file looks like this:

```.yml
OPENAI_API_KEY: "XXX-...-XXX"
``` 

3. Install the appropriate package:
```{R}
#| eval: false
install.packages("openai")
```


4. Try running the example below: 


```{R}
# Specify the path to your API-credentials file
library(openai) # open-source, not official implementation!
library(yaml) # For parsing the API key
library(glue) # For easy prompt formating
library(jsonlite) # For parsing .json formatted responses
file_path <- "../../../credentials.yml"
credentials <- yaml.load_file(file_path)
OPENAI_API_KEY <- credentials$OPENAI_API_KEY


# Get a model response
response <- create_chat_completion(
  model = "gpt-3.5-turbo",
  messages = list(
    list(
      "role" = "system",
      "content" = "
                        You are playing the role of a radio operator
                        waiting to receive a signal from the user.
                        When you receive a message, reply with much enthousiasm and gusto.
                        "
    ),
    list(
      "role" = "user",
      "content" = "This is OPENAI-API-USER, can anybody hear me?"
    )
  ),
  temperature = 1, # The randomness setting of the model: 0 is deterministic, 2 is most random
  n = 1, # How many completions do we want the model to generate
  max_tokens = NULL, # How many tokens is the model allowed to use,
  openai_api_key = OPENAI_API_KEY # Your secret key to the API
)
print(response$choices[["message.content"]])
```

# Exercise: Getting to know the chat completion API
Experiment with the various parameters of the chat completion API:

```{r}
# We'll define a helper function to make this interaction more easy
system_prompt <- "You are a helpful assistant."
user_prompt <- "
Task: Explain to me what a Transformer model is.
Task specification: Assume that I know nothing about NLP. I am a highschool student interested in this technology.
I understand basic math, but please use as little as possible. Please give a short answer of at most 4 senteces
"

generate_completion <- function(system = system_prompt,
                                prompt = user_prompt,
                                t = 1,
                                n = 1,
                                max_tokens = 500,
                                openai_api_key = OPENAI_API_KEY) {
  #' Flexible API Request Function with Integrated Glue Functionality
  #'
  #' This function sends a request to a specified API, integrating the 'glue' function to allow
  #' dynamic insertion of context documents into the user_prompt.
  #'
  #' @param system the system prompt provided to the model The URL of the API to which the request is being sent, defaults to system_prompt object
  #' @param prompt the user prompt provided to the model, defaults to user_prompt object
  #' @param t the temperature setting for the mode, defaults to 1
  #' @param n the number of completions to generate, defaults to 1
  #' @param max_tokens the maximum model response length, defaults to 500 tokens
  #'
  #' @return
  #' The response object from the chat_completion api
  response <- create_chat_completion(
    model = "gpt-3.5-turbo",
    messages = list(
      list(
        "role" = "system",
        "content" = system_prompt
      ),
      list(
        "role" = "user",
        "content" = glue(user_prompt)
      )
    ),
    temperature = t, # The randomness setting of the model: 0 is deterministic, 2 is most random
    n = n, # How many completions do we want the model to generate
    max_tokens = max_tokens, # How many tokens is the model allowed to use,
    openai_api_key = openai_api_key # Your secret key to the API
  )
  for (choice_num in 1:length(response$choice[["index"]])) {
    cat(glue('**Response {choice_num}**:\n\n {response$choices[[choice_num, "message.content"]]}\n\n\n\n'))
  }
  return(response)
}
```

-   What happens when you adjust the temperature?

```{R}
response <- generate_completion(t = 0.1)
```
*At first glance there is not that much noticeable when I set the value lower.

```{R}
response <- generate_completion(t = 1.5)
```

```{R}
response <- generate_completion(t = 1.7)
```

*With these very high values the output becomes very creative. There is more variety, but it might not fit the prompt anymore.*

-   What happens when you adjust n? Set it to 3 for example. \textcolor{red}{Don't set this value high as a component of the API cost scales lineairly with this parameter!}

*The model will generate n outputs for the given input. Below this illustrates how the temperature effects the model output.*

```{R}
response <- generate_completion(t = 0.5, n = 3)
```

```{R}
response <- generate_completion(t = 1, n = 3)
```

```{R}
response <- generate_completion(t = 1.5, n = 3)
```

```{R}
response <- generate_completion(t = 1.75, n = 3)
```

-   What happens when you adjust max_tokens? 

```{R}
response <- generate_completion(t = 1, n = 1, max_tokens = 10)
```

*The model just stops generating the completion as soon as the limit is reached.*

# Exercise: Iterative prompt development on a movie review
Now that we have acces to the API, it becomes a lot easier to craft better prompts. We can set the temperature for our chat completion to 0, which ensures that the same prompt will gave the same result every time.

For each of the tasks below, finetune your prompt untill you are happy with the result. Each task adds to the task before. 

## Load the review
It can be found under `./Day 1/exercises/data/movie-review-1.txt`

```{R}
read_txt <- function(file_path) {
  # Read the file
  review <- readLines(file_path, warn = FALSE)

  # Combine the seperate lines into a single line
  review <- paste(review, collapse = "\n")

  return(review)
}

# Define the file path
file_path_review1 <- "../data/movie-review-1.txt"

review <- read_txt(file_path_review1)
cat(review)
```


## Task: Provide a synopsis of the review
```{R}
system_prompt <- "You are a helpful assistant who's role is to help the user parse movie reviews"
user_prompt <- "
  Task: Your task is to provide a brief synopsis of the movie presented to you below.
  Make sure you capture the most important elements of the review.

  Review:
  '''
  {review}
  '''
  "


response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```

## Task: Make sure the synopsis of the review is at most 3 sentences long
```{R}
system_prompt <- "You are a helpful assistant who's role is to help the user parse movie reviews"
user_prompt <- "
  Task: Your task is to provide a brief synopsis of the movie presented to you below.
  Make sure you capture the most important elements of the review, using at most 3 sentences.

  Review:
  '''
  {review}
  '''
  "


response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```


## Task: Also extract the movie title, genre, year, director and actors from the review. Place this information at the top of the response.

```{R}
system_prompt <- "You are a helpful assistant who's role is to help the user parse movie reviews"
user_prompt <- "
  Task: Your task is to extract relevant information from the movie review provided below.

  Begin by extracting the title, genre, year director and actors from the movie review.

  Next, provide a brief synopsis of the movie presented to you below.
  Make sure you capture the most important elements of the review, using at most 3 sentences.

  Review:
  '''
  {review}
  '''
  "


response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```


## Task: Also extract a list of tips and tops of the movie, using only keywords

```{R}
system_prompt <- "You are a helpful assistant who's role is to help the user parse movie reviews"
user_prompt <- "
  Task: Your task is to extract relevant information from the movie review provided below.

  Begin by extracting the title, genre, year director and actors from the movie review.

  Next, provide a brief synopsis of the movie presented to you below.
  Make sure you capture the most important elements of the review, using at most 3 sentences.

  End your response with a section called tops: a list of keywords for the best elements of the movie according to the review,
  followed by a similair section called tips: a list of keywords for the worst elements of the movie according to the review.
  Provide at most 3 tops and 3 tips.

  Review:
  '''
  {review}
  '''
  "

response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```
*Notice that for the tips and tops it is not fully listening to our task specification of only using keywords.*

## Task: Make the model return these fields in a nicely structured (and valid) .json format

```{R}
system_prompt <- "You are a helpful assistant who's role is to help the user parse movie reviews"
user_prompt <- "
  Task: Your task is to extract relevant information from the movie review provided below.

  Begin by extracting the title, genre, year director and actors from the movie review.

  Next, provide a brief synopsis of the movie presented to you below.
  Make sure you capture the most important elements of the review, using at most 3 sentences.

  End your response with a section called tops: a list of keywords for the best elements of the movie according to the review,
  followed by a similair section called tips: a list of keywords for the worst elements of the movie according to the review.
  Provide at most 3 tops and 3 tips.

  Structure the output a correctly structured .json format with the following fields:
  Title
  Genre
  Year
  Director
  Actors
  Synopsis
  Tops
  Tips

  Review:
  '''
  {review}
  '''
  "


response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```
*We can now parse this response with the jsonlite package and acces specific elements of the generated response*
```{R}
parsed_response <- fromJSON(response$choices$message.content)
```
```{R}
parsed_response$Title
```
```{R}
parsed_response$Synopsis
```

```{R}
parsed_response$Tips[1]
```

## Task: Review number 2
Now apply the prompt you've crafted to the second movie review:\
`./Day 1/exercises/data/movie-review-2.txt`\
Does it work?

```{R}
# Define the file path
file_path_review2 <- "../data/movie-review-2.txt"

review <- read_txt(file_path_review2)
cat(review)
```

```{R}
response <- generate_completion(
  system = system_prompt,
  prompt = user_prompt,
  t = 0,
  n = 1
)
```

*No problem, you can see the model has extracted the same elements from the review. In practice you want to try more test case and perform actual validation, but this will be discussed more on day 2.*

# Exercise: Estimate token usage before sending your prompts
Before you send messages to the OpenAI API, you may want to check how long they are, you can do this using the functions below:

**R**\
For R, we need to make an estimate, as the tokenizer has no R implementation. 
We use the rule of thumb that a single word is $\frac{4}{3}$ of a token. In practice this works quite well. In a few test cases the value returned was typically within $\sim5\%$ of the true value.

```{R}
num_tokens_from_messages <- function(messages, model = "gpt-3.5-turbo-0613", tokens_per_word = 4 / 3) {
  tokens_per_message <- 0
  tokens_per_name <- 0

  if (model %in% c(
    "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613", "gpt-4-0314",
    "gpt-4-32k-0314", "gpt-4-0613", "gpt-4-32k-0613"
  )) {
    tokens_per_message <- 3
    tokens_per_name <- 1
  } else if (model == "gpt-3.5-turbo-0301") {
    tokens_per_message <- 4
    tokens_per_name <- -1
  } else if (grepl("gpt-3.5-turbo", model)) {
    message("Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.")
    return(num_tokens_from_messages(messages, model = "gpt-3.5-turbo-0613"))
  } else if (grepl("gpt-4", model)) {
    message("Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.")
    return(num_tokens_from_messages(messages, model = "gpt-4-0613"))
  } else {
    stop(sprintf("num_tokens_from_messages() is not implemented for model %s. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.", model))
  }

  num_tokens <- 0

  for (message in messages) {
    num_tokens <- num_tokens + tokens_per_message
    for (key in names(message)) {
      value <- message[[key]]
      # Count multiple spaces as a single space.
      value <- gsub(" +", " ", value)
      # Calculate tokens for each word, assuming each word is 4/3 tokens for English
      num_tokens <- num_tokens + (length(strsplit(value, " ")[[1]]) * tokens_per_word)
      if (key == "name") {
        num_tokens <- num_tokens + tokens_per_name
      }
    }
  }

  num_tokens <- num_tokens + 3 # every reply is primed with assistant
  return(num_tokens)
}
```
compare the output of the functions below with the information returned in the response object from the previous exercise. 

```{R}
glue("In the previous exercise, the api told us we sent: {response$usage$prompt_tokens}")
```

```{R}
messages <- list(
  list(
    "role" = "system",
    "content" = system_prompt
  ),
  list(
    "role" = "user",
    "content" = glue(user_prompt)
  )
)

estimated_tokens <- num_tokens_from_messages(messages)
glue("Using the above function, for the input we found: {estimated_tokens}")
```

```{R}
diff <- round((estimated_tokens - response$usage$prompt_tokens) / response$usage$prompt_token * 100, 2)
glue("That means the difference between the true value and the estimated value was {diff}%")
```


## API-costs
We are not just interested in how many tokens we use, we are interested in how many cents we spend on a prompt. Write a function that uses the function above to calculate the cost of a prompt. Return 2 values:

- The estimated cost of the prompt input
- The estimated cost of the prompt output

For this exercise you may assume that the output is 4/5 of the length of the input. This value is an arbitrary value chosen for this exercise.

```{R}
api_cost_estimate <- function(messages) {
  estimated_input_tokens <- num_tokens_from_messages(messages)
  estimated_output_tokens <- round(4 / 5 * estimated_input_tokens, 0)
  price_input <- estimated_input_tokens / 1000 * 0.001 # $0.0010 / 1K tokens
  price_output <- estimated_output_tokens / 1000 * 0.002 # $0.0020 / 1K tokens
  total <- price_input + price_output
  return(total)
}
api_cost_estimate(messages)
```

*That implies that the above prompt costs 0.002 dollar every time we send it to the API.*
*In practice it was lower, as the completion was shorter at `r response$usage$completion_tokens` tokens*
