{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aKYuunTBmgN"
      },
      "source": [
        "# Notebook: NLP with LLMs\n",
        "---\n",
        "\n",
        "Part of the [Masterclass: Large Language Models for Data Science](https://github.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science)\n",
        "![](https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/main/slides/day_1/figures/pair-programming-with-llms.webp)\n",
        "by Alex van Vorstenbosch\n",
        "---\n",
        "\n",
        "Check out the prompting cheatsheet for prompt inspiration: **Cheatsheet.pdf**!\\\n",
        "It can be found in the course material under **./slides/**.\n",
        "\n",
        "It contains some example prompts and inspiration for all kinds of prompting tasks. Read it before starting on the assingments below.\n",
        "\n",
        "If you want some more information about google colab and the different LLM models you can use, check out the **[general colab notebook](https://colab.research.google.com/drive/1LnsvqzL8BjO3m52siYJpakS-uR64B9pI?usp=sharing)**\n",
        "\n",
        "\n",
        "# Sentiment Analysis\n",
        "\n",
        "In this Exercise set, we are going to use an LLM for a sentiment analysis problem. We'll be using some functions and lessons learnt in previous lectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwslO3-sjTw"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "First we will need to install the necessary python packages.\n",
        "Luckily for us, google colab comes with most of the libraries and requiered cuda software already pre-installed.\n",
        "\n",
        "## 1.1 Runtime\n",
        "---\n",
        "\n",
        "We will want to use a GPU to run the examples in this notebook. In Google Colab, go to the menu bar:\n",
        "\n",
        "\n",
        "**Menu bar > Runtime > Change runtime type > T4 GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Install packages\n",
        "Run the cell below to install `llama-cpp-python` which allows fast inference on GPU and CPU with GGUF quantized models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% capture\n",
        "!pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ],
      "metadata": {
        "id": "iV7MV8m6vQR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Load libraries"
      ],
      "metadata": {
        "id": "sJpDj2H4ECXU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A39L-5wBmgO"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from llama_cpp.llama import Llama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Load helper functions from github\n",
        "\n",
        "In the repo I have included 2 helper functions for talking to LLMs:\n",
        "\n",
        "1. generate_response\\\n",
        "   Generate a response given a set of chat messages, with optional streaming behavior.\n",
        "\n",
        "2. interactive_chat\\\n",
        "   Allows the user to engage in an interactive chat session with the model (streaming by default).\n"
      ],
      "metadata": {
        "id": "yBWj-Yy1H8OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o helper_functions.py https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/course_2025/exercises/day_1/helper_functions/helper_functions.py"
      ],
      "metadata": {
        "id": "WT87Yme-IFuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import *\n"
      ],
      "metadata": {
        "id": "WNsT3b7kIRGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4w7gVsBmgP"
      },
      "source": [
        "## 2 Read the data\n",
        "The dataset contains 500 sentences of the SemEval2014-restaurants dataset. This is a dataset for ABSA:\\\n",
        "\"Aspect Based Sentiment Analysis\": Given all the aspects (red: relevant elements) detect the sentiment expressed about them.\n",
        "\n",
        "There are 2 flavours of ABSA:\n",
        "\n",
        "- regular ABSA: Aspect Based Sentiment Analysis - Detect the sentiment for each aspect given.\n",
        "- E2E-ABSA: End-to-End Aspect Based Sentiment Analysis - Detect both the aspects and their sentiment.\n",
        "\n",
        "E2E is significantly harder to achieve, and to a higher degree depends on the labeling quality of the data set.\n",
        "For this exercise set, we will be performing regular ABSA using an LLM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2427bqBmgP"
      },
      "source": [
        "# Download the data onto google colab\n",
        "!curl --create-dirs -O --output-dir  ./data/ https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/gh_pages/slides/Day_2/exercises/data/semEval2014-restaurants-x.csv\n",
        "!curl -O --output-dir ./data/ https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/gh_pages/slides/Day_2/exercises/data/semEval2014-restaurants-y.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmGTN4ixBmgQ"
      },
      "source": [
        "# Read the data\n",
        "data_x = pd.read_csv(\"./data/semEval2014-restaurants-x.csv\", delimiter=\";\")\n",
        "data_y = pd.read_csv(\"./data/semEval2014-restaurants-y.csv\", delimiter=\";\")\n",
        "\n",
        "\n",
        "# DON'T TOUCH THIS CODE ---------------------##\n",
        "# We need everybodies data_test to be equal  ##\n",
        "# --------------------------------------------##\n",
        "# Not all sentences in the dataset have a y-label in the original data set\n",
        "# in this cleaned sample they do\n",
        "data = pd.merge(data_x, data_y, on=\"sentence_id\", how=\"inner\")\n",
        "\n",
        "# Add a new column 'new_id' for grouping\n",
        "data[\"new_id\"] = data.groupby(\"sentence_id\").ngroup()\n",
        "\n",
        "# Create data_train and data_test DataFrames\n",
        "data_train = data[data[\"new_id\"] <= 400].drop(columns=\"new_id\")\n",
        "data_test = data[data[\"new_id\"] > 400].drop(columns=\"new_id\")\n",
        "\n",
        "# --------------------------------------------##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8XJrK7dBmgQ"
      },
      "source": [
        "### Check out the data, what does it look like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yxwZj44BmgQ"
      },
      "source": [
        "# <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zu8ASQGBmgR"
      },
      "source": [
        "# 3 ABSA\n",
        "\n",
        "Here we will try our hand at sentiment analysis using an out-of-the-box LLM.\\\n",
        "\\\n",
        "*IMPORTANT-RULE:*\\\n",
        "You are allowed to do whatever you want with the training data.\n",
        "Play with it, experiment with it, etc. But you are only allowed to touch the test-data when we evaluate our model performance.\n",
        "If you need to alter the format of data_test for your prompt, that is allowed, but other than that, you cannot touch it!\n",
        "We will see who get's the highest test-performance at the end of the session.\n",
        "\n",
        "## 3.1 Zero-shot - Single\n",
        "\n",
        "At this point, we are going to try performing Zero-shot sentiment analysis. The rules are simple:\n",
        "\n",
        "- The model response should contain the prediction for all aspects in the same sentence at once.\n",
        "- You are not allowed to provide any sentiment analysis examples to the model.\n",
        "- You are allowed to describe the various sentiment polarities and their definition.\n",
        "- You are free to write this prompt as you see fit, just don't give examples.\n",
        "- You can decide how to distribute the prompt over the system_prompt and the user_prompt.\n",
        "- Check out the labeling rules guide for SemEval: [SemEval14_ABSA_AnnotationGuidelines.pdf](https://alt.qcri.org/semeval2014/task4/data/uploads/semeval14_absa_annotationguidelines.pdf)\n",
        "- Make it easy for yourself: use the json mode for easy extraction of the classifications!\n",
        "\n",
        "**Example:**\\\n",
        "Here is an example to demonstrate what the data looks like, and what the expected output is.\n",
        "\n",
        "Targets:\n",
        "\n",
        "- sentence: \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\"\n",
        "- aspects: food, kitchen, menu\n",
        "- sentiment: positive, positive, neutral\n",
        "\n",
        "Output:\n",
        "\n",
        "- {\"food\": \"positive\", \"kitchen\": \"positive\", \"menu\": \"neutral\"}\n",
        "\n",
        "Let us start with classifying the sentiment for a single example sentence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Uncomment and run this cell if you need to clear the GPU memory!\n",
        "# import gc\n",
        "# import torch\n",
        "# del llm\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Mv02oU7Od04W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load you llm model\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=32518, # this is 50 A4 pages of context window!\n",
        "    verbose=False,\n",
        "    logits_all=True\n",
        ")"
      ],
      "metadata": {
        "id": "PDPmy_fKdBEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMN2_v9HBmgR"
      },
      "source": [
        "# this notation is a bit weird\n",
        "# It's a trick to delay the f string evaluation\n",
        "# Now you can call user_prompt() within a loop to get the current correct values\n",
        "# We only apply this to the user_prompt as the system_prompt should not change between different inputs.\n",
        "system_prompt = \"\"\"\n",
        "# <- Your input here ->\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = lambda: f\"\"\"\n",
        "# <- Your input here ->\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwM1i5qBmgR"
      },
      "source": [
        "# use these variables in your F-string\n",
        "i = 1\n",
        "x_sentence_id = data_train.loc[i, \"sentence_id\"]\n",
        "target_sentence = data_train.loc[i, \"sentence_text\"]\n",
        "\n",
        "targets = data_train[data_train['sentence_id'] == x_sentence_id]\n",
        "target_aspect_terms_combined = \", \".join(targets.aspect_term)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__f9olhPBmgR"
      },
      "source": [
        "# Generate a classification\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt()}\n",
        "]\n",
        "response_raw = generate_response(llm, messages, json_mode=True, temperature=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEM7OvPoBmgS"
      },
      "source": [
        "### Check if the predictions where correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvyoQKYsBmgS"
      },
      "source": [
        "# Read classifications from response\n",
        "response_classification = json.loads(response_raw)\n",
        "\n",
        "# Check correctness for each prediction\n",
        "# <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnfjgsIvBmgT"
      },
      "source": [
        "You can play around a bit here, to test your prompt on individual examples. In the next section, I've supplied code you can use to test your prompt on multiple examples at once.\n",
        "\n",
        "## Zero-shot - Multiple examples\n",
        "\n",
        "Now we'll do a bigger trial run on the training data. Below I've written some code that puts all the data of the first 50 sentences into a single\n",
        "dataframe and runs predictions. You should finish the `get_predicitions` function to get predictions for every sentence in data_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG7lD3BTBmgT"
      },
      "source": [
        "n_samples = # <- Your input here -> # Select your number\n",
        "\n",
        "# Add a new column 'new_id' for grouping\n",
        "data_train_sample = data_train\n",
        "data_train_sample['new_id'] = data_train_sample.groupby('sentence_id').ngroup()\n",
        "\n",
        "# Create data_train_sample_train and data_train_sample_test data_train_sampleFrames\n",
        "data_train_sample= data_train_sample[data_train_sample['new_id'] <= n_samples].drop(columns='new_id')\n",
        "\n",
        "def predict_sentiment(system_prompt = system_prompt, user_prompt = user_prompt):\n",
        "  #' A simple wrapper function for generate completion that extracts the sentiment from the response\n",
        "  #' Please modify to your own liking!\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt()}\n",
        "  ]\n",
        "  response = generate_response(llm, messages, json_mode=True, temperature=0)\n",
        "  return(json.loads(response))\n",
        "\n",
        "\n",
        "# We're making predictions in a for loop, and appending them to a dataframe\n",
        "# Not elegant or vectorised, but easy to understand.\n",
        "# please note: running this takes a while to finish...\n",
        "sentence_ids = data_train_sample['sentence_id'].unique()\n",
        "predictions_train = pd.DataFrame(columns=['sentence_id', 'aspect_term', 'polarity_prediction'])\n",
        "\n",
        "for target_sentence_id in sentence_ids:\n",
        "    targets = data_train_sample[data_train_sample['sentence_id'] == target_sentence_id]\n",
        "    target_sentence = targets.iloc[0]['sentence_text']\n",
        "\n",
        "    target_aspect_terms_combined = ', '.join(targets['aspect_term'])\n",
        "\n",
        "    # Assuming predict_sentiment is a function you have defined\n",
        "    prediction = predict_sentiment(system_prompt, user_prompt)\n",
        "\n",
        "    for aspect, polarity in prediction.items():\n",
        "        temp_df = pd.DataFrame({\n",
        "            'sentence_id': [target_sentence_id],\n",
        "            'aspect_term': [aspect],\n",
        "            'polarity_prediction': [polarity]\n",
        "        })\n",
        "        predictions_train = pd.concat([predictions_train, temp_df], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK76zDtLBmgT"
      },
      "source": [
        "Bring them together so we can evaluate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a89He95-BmgT"
      },
      "source": [
        "data_train_p = pd.merge(\n",
        "    data_train_sample, predictions_train, how=\"left\", on=[\"sentence_id\", \"aspect_term\"]\n",
        ").dropna()\n",
        "\n",
        "# Drop predictions that are not formatted correctly\n",
        "data_train_p = data_train_p[data_train_p[\"polarity_prediction\"].apply(lambda x: isinstance(x, str))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py3kv7EgBmgT"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "In this section you can evaluate the performance of your model. You will get:\n",
        "\n",
        "- The accuracy\n",
        "- Class level: Precision, Recall, Specificity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXI0qM_rBmgU"
      },
      "source": [
        "# get true and false\n",
        "y_true = data_train_p[\"polarity\"]\n",
        "y_pred = data_train_p[\"polarity_prediction\"]\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Extracting metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the baseline accuracy\n",
        "baseline_accuracy = data_train[\"polarity\"].value_counts().max() / len(data_train)\n",
        "\n",
        "# Print the metrics\n",
        "print(\n",
        "    f\"The baseline accuracy (predicting majority class): {round(baseline_accuracy, 3)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Micro Accuracy = Micro Precision = Micro Recall = Micro F1: {round(accuracy, 3)}\"\n",
        ")\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "print(classification_report(y_true, y_pred, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaS3JlTBmgU"
      },
      "source": [
        "### Confusion matrix\n",
        "\n",
        "This code takes as input your confusion_matrix to turn it into a plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu84HRAnBmgU"
      },
      "source": [
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=conf_matrix,\n",
        "    display_labels=[\n",
        "        \"conflict\",\n",
        "        \"negative\",\n",
        "        \"neutral\",\n",
        "        \"positive\",\n",
        "    ],\n",
        ")\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJtluOW3BmgU"
      },
      "source": [
        "### Inspect the wrong predictions\n",
        "\n",
        "Please check what the examples where the model got wrong.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj23WkFfBmgV"
      },
      "source": [
        "# <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB7b-TitBmgV"
      },
      "source": [
        "Please check if there were examples where the model returned the wrong aspect label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkVd6UIBmgV"
      },
      "source": [
        "# <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCuOerEmBmgV"
      },
      "source": [
        "The model corrects misspelled words in the current version, and it sometimes splits terms into 2 or more terms where it shouldn't\n",
        "\n",
        "## Zero-shot - Test\n",
        "You can still iterate over your zero-shot prompt above, using all the information from the train set, or using a larger sample of the training-data.\n",
        "If you are ready to test yourself, run the cell below to evaluate your model on the test-set.\n",
        "\n",
        "**YOU ARE ONLY ALLOWED TO DO THIS ONCE, THIS WILL BE YOUR FINAL SCORE ON THE ZERO-SHOT EXERCISE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws-WR1yiBmgV"
      },
      "source": [
        "# We're making predictions in a for loop, and appending them to a dataframe\n",
        "# Not elegant or vectorised, but easy to understand.\n",
        "# please note: running this takes a while to finish...\n",
        "\n",
        "sentence_ids = data_test[\"sentence_id\"].unique()\n",
        "predictions_test = pd.DataFrame(\n",
        "    columns=[\"sentence_id\", \"aspect_term\", \"polarity_prediction\"]\n",
        ")\n",
        "\n",
        "for target_sentence_id in sentence_ids:\n",
        "    targets = data_test[data_test[\"sentence_id\"] == target_sentence_id]\n",
        "    target_sentence = targets.iloc[0][\"sentence_text\"]\n",
        "\n",
        "    target_aspect_terms_combined = \", \".join(targets[\"aspect_term\"])\n",
        "\n",
        "    # Assuming predict_sentiment is a function you have defined\n",
        "    prediction = predict_sentiment(system_prompt, user_prompt)\n",
        "\n",
        "    for aspect, polarity in prediction.items():\n",
        "        temp_df = pd.DataFrame(\n",
        "            {\n",
        "                \"sentence_id\": [target_sentence_id],\n",
        "                \"aspect_term\": [aspect],\n",
        "                \"polarity_prediction\": [polarity],\n",
        "            }\n",
        "        )\n",
        "        predictions_test = pd.concat([predictions_test, temp_df], ignore_index=True)\n",
        "\n",
        "data_test_p = pd.merge(\n",
        "    data_test, predictions_test, how=\"left\", on=[\"sentence_id\", \"aspect_term\"]\n",
        ").dropna()\n",
        "\n",
        "# get true and false\n",
        "y_true = data_test_p[\"polarity\"]\n",
        "y_pred = data_test_p[\"polarity_prediction\"]\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Extracting metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(\n",
        "    f\"The baseline accuracy (predicting majority class): {round(baseline_accuracy, 3)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Micro Accuracy = Micro Precision = Micro Recall = Micro F1: {round(accuracy, 3)}\"\n",
        ")\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=conf_matrix,\n",
        "    display_labels=[\n",
        "        \"conflict\",\n",
        "        \"negative\",\n",
        "        \"neutral\",\n",
        "        \"positive\",\n",
        "    ],\n",
        ")\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIn9tgNwBmgV"
      },
      "source": [
        "## Few-shot - Training\n",
        "\n",
        "Now adjust your system prompt, use examples to demonstrate how labels should be given.\n",
        "You can only use examples from the training set, or examples that you thought of yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-MbK7NOBmgW"
      },
      "source": [
        "def predict_sentiment(system_prompt = system_prompt, user_prompt = user_prompt):\n",
        "    #' A simple wrapper function for generate completion that extracts the sentiment from the response\n",
        "    #' Please modify to your own liking!\n",
        "    messages=[\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": system_prompt,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\":\"\"\"\n",
        "    <- Your input here ->\n",
        "    \"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\":\"\"\"\n",
        "    # <- Your input here ->\n",
        "      \"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\":\"\"\"\n",
        "    # <- Your input here ->\n",
        "    \"\"\"\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\":\"\"\"\n",
        "    # <- Your input here ->\n",
        "    \"\"\",\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": user_prompt()\n",
        "      },\n",
        "    ]\n",
        "    response = generate_response(llm, messages, json_mode=True, temperature=0)\n",
        "    return(json.loads(response))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngy6uCftBmgW"
      },
      "source": [
        "Here is the same code as before to make predictions, but now you should:\n",
        "\n",
        "- make sure that you updated the `predict_sentiment` function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdkMoGh0BmgW"
      },
      "source": [
        "n_samples = # <- Your input here -> # Select your number\n",
        "\n",
        "# Add a new column 'new_id' for grouping\n",
        "data_train_sample = data_train\n",
        "data_train_sample['new_id'] = data_train_sample.groupby('sentence_id').ngroup()\n",
        "\n",
        "# Create data_train_sample_train and data_train_sample_test data_train_sampleFrames\n",
        "data_train_sample= data_train_sample[data_train_sample['new_id'] <= n_samples].drop(columns='new_id')\n",
        "\n",
        "# We're making predictions in a for loop, and appending them to a dataframe\n",
        "# Not elegant or vectorised, but easy to understand.\n",
        "# please note: running this takes a while to finish...\n",
        "sentence_ids = data_train_sample['sentence_id'].unique()\n",
        "predictions_train = pd.DataFrame(columns=['sentence_id', 'aspect_term', 'polarity_prediction'])\n",
        "\n",
        "for target_sentence_id in sentence_ids:\n",
        "    targets = data_train_sample[data_train_sample['sentence_id'] == target_sentence_id]\n",
        "    target_sentence = targets.iloc[0]['sentence_text']\n",
        "\n",
        "    target_aspect_terms_combined = ', '.join(targets['aspect_term'])\n",
        "\n",
        "    # Assuming predict_sentiment is a function you have defined\n",
        "    prediction = predict_sentiment(system_prompt, user_prompt)\n",
        "\n",
        "    for aspect, polarity in prediction.items():\n",
        "        temp_df = pd.DataFrame({\n",
        "            'sentence_id': [target_sentence_id],\n",
        "            'aspect_term': [aspect],\n",
        "            'polarity_prediction': [polarity]\n",
        "        })\n",
        "        predictions_train = pd.concat([predictions_train, temp_df], ignore_index=True)\n",
        "\n",
        "data_train_p = pd.merge(data_train_sample, predictions_train, how=\"left\", on = [\"sentence_id\", \"aspect_term\"]).dropna()\n",
        "\n",
        "# Drop predictions that are not formatted correctly\n",
        "data_train_p = data_train_p[data_train_p[\"polarity_prediction\"].apply(lambda x: isinstance(x, str))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_p = pd.merge(\n",
        "    data_train_sample, predictions_train, how=\"left\", on=[\"sentence_id\", \"aspect_term\"]\n",
        ").dropna()\n",
        "\n",
        "# Drop predictions that are not formatted correctly\n",
        "data_train_p = data_train_p[data_train_p[\"polarity_prediction\"].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# get true and false\n",
        "y_true = data_train_p[\"polarity\"]\n",
        "y_pred = data_train_p[\"polarity_prediction\"]\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Extracting metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculate the baseline accuracy\n",
        "baseline_accuracy = data_train[\"polarity\"].value_counts().max() / len(data_train)\n",
        "\n",
        "# Print the metrics\n",
        "print(\n",
        "    f\"The baseline accuracy (predicting majority class): {round(baseline_accuracy, 3)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Micro Accuracy = Micro Precision = Micro Recall = Micro F1: {round(accuracy, 3)}\"\n",
        ")\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "print(classification_report(y_true, y_pred, digits=3))"
      ],
      "metadata": {
        "id": "vrGLTtBobUM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=conf_matrix,\n",
        "    display_labels=[\n",
        "        \"conflict\",\n",
        "        \"negative\",\n",
        "        \"neutral\",\n",
        "        \"positive\",\n",
        "    ],\n",
        ")\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPCIkbU1b8yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNH1HU7BmgX"
      },
      "source": [
        "## Few-shot - Test\n",
        "You can still iterate over your few-shot prompt above, using all the information from the train set, or using a larger sample of the training-data.\n",
        "If you are ready to test yourself, run the cell below to evaluate your model on the test-set.\n",
        "\n",
        "**YOU ARE ONLY ALLOWED TO DO THIS ONCE, THIS WILL BE YOUR FINAL SCORE ON THE FEW-SHOT EXERCISE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh_aU0D6BmgX"
      },
      "source": [
        "# We're making predictions in a for loop, and appending them to a dataframe\n",
        "# Not elegant or vectorised, but easy to understand.\n",
        "# please note: running this takes a while to finish...\n",
        "\n",
        "sentence_ids = data_test[\"sentence_id\"].unique()\n",
        "predictions_test = pd.DataFrame(\n",
        "    columns=[\"sentence_id\", \"aspect_term\", \"polarity_prediction\"]\n",
        ")\n",
        "\n",
        "for target_sentence_id in sentence_ids:\n",
        "    targets = data_test[data_test[\"sentence_id\"] == target_sentence_id]\n",
        "    target_sentence = targets.iloc[0][\"sentence_text\"]\n",
        "\n",
        "    target_aspect_terms_combined = \", \".join(targets[\"aspect_term\"])\n",
        "\n",
        "    # Assuming predict_sentiment is a function you have defined\n",
        "    prediction = predict_sentiment(system_prompt, user_prompt)\n",
        "\n",
        "    for aspect, polarity in prediction.items():\n",
        "        temp_df = pd.DataFrame(\n",
        "            {\n",
        "                \"sentence_id\": [target_sentence_id],\n",
        "                \"aspect_term\": [aspect],\n",
        "                \"polarity_prediction\": [polarity],\n",
        "            }\n",
        "        )\n",
        "        predictions_test = pd.concat([predictions_test, temp_df], ignore_index=True)\n",
        "\n",
        "data_test_p = pd.merge(\n",
        "    data_test, predictions_test, how=\"left\", on=[\"sentence_id\", \"aspect_term\"]\n",
        ").dropna()\n",
        "\n",
        "# get true and false\n",
        "y_true = data_test_p[\"polarity\"]\n",
        "y_pred = data_test_p[\"polarity_prediction\"]\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Extracting metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(\n",
        "    f\"The baseline accuracy (predicting majority class): {round(baseline_accuracy, 3)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Micro Accuracy = Micro Precision = Micro Recall = Micro F1: {round(accuracy, 3)}\"\n",
        ")\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=conf_matrix,\n",
        "    display_labels=[\n",
        "        \"conflict\",\n",
        "        \"negative\",\n",
        "        \"neutral\",\n",
        "        \"positive\",\n",
        "    ],\n",
        ")\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}