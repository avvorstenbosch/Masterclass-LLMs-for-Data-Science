{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook: Retrieval Augmented Generation (RAG) for Question Answering\n",
        "---\n",
        "\n",
        "Part of the [Masterclass: Large Language Models for Data Science](https://github.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science)\n",
        "![](https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/main/slides/Day_1/figures/pair-programming-with-llms.webp)\n",
        "by Alex van Vorstenbosch\n",
        "---\n",
        "\n",
        "Check out the prompting cheatsheet for prompt inspiration: **Cheatsheet.pdf**!\\\n",
        "It can be found in the course material under **./slides/**.\n",
        "\n",
        "It contains some example prompts and inspiration for all kinds of prompting tasks. Read it before starting on the assingments below.\n",
        "\n",
        "If you want some more information about google colab and the different LLM models you can use, check out the **[general colab notebook](https://colab.research.google.com/drive/1LnsvqzL8BjO3m52siYJpakS-uR64B9pI?usp=sharing)**\n",
        "\n",
        "\n",
        "# Context based question answering\n",
        "\n",
        "In this notebook we will build a system that can answer question based on context retreived from your corpus of documents"
      ],
      "metadata": {
        "id": "ajDfou1m9ls_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg7S6rX2MHTV"
      },
      "source": [
        "This is where we bring it all together.\n",
        "The past days we've learned about:\n",
        "\n",
        "-   Large Language Models\n",
        "-   Their strengths and weaknesses\n",
        "-   Embeddings\n",
        "-   Prompting\n",
        "-   NLP\n",
        "\n",
        "We will bring this all together in this supercharged tutorial/exercise set.\n",
        "\n",
        "# Search + Generation\n",
        "\n",
        "Large Language Models have a vast amount of internal knowledge. This internal knowledge is known as parametric knowledge.\n",
        "This constitutes the general linguistic knowledge these models have, as well as the knowledge they have about the world in general.\n",
        "However, you already know that this parametric knowledge is far from flawless.\n",
        "These models tend to hallucinate more, the more detailed your requests become. Also, these models have no reference of what is true and what is not.\n",
        "Truth and factualness is fundamentally not a core component in the training of these models, they just don't know.   \n",
        "Furthermore, when using thse models you may have questions about:\n",
        "\n",
        "-   Recent events\n",
        "-   Proprietary documents\n",
        "-   Private documents\n",
        "\n",
        "Per definition these were not included in the training data.\n",
        "In this case, we need to feed the model with our own knowledge base such that it may have the ability to answer our questions.\n",
        "This works wonderfully because we can make use of the strong reasoning capabilities these models display on tokens that are directly accesible in memory (their context window).\n",
        "You may liken this to an open book exam: Long term memory is flawed (also in humans) but by providing the model with direct reference texts we can supercharge the question answering capabilities, while allowing ourselves to make the exam (our questions) more specific and more difficult to answer.\n",
        "\n",
        "# Dataset\n",
        "\n",
        "For this assignment we will be working with a Wikipedia dataset that covers the category: [\"Natural Language Processing\"](https://en.wikipedia.org/wiki/Category:Natural_language_processing).\n",
        "From this Wikipedia category a grand total of 964 articles have been extracted and parsed into plain-text.\n",
        "The sections per page were filtered such that we should have retained only the most informative sections per article.\n",
        "This means we filtered out the following sections:\n",
        "\n",
        "-   See also\n",
        "-   References\n",
        "-   External links\n",
        "-   Further reading\n",
        "-   Footnotes\n",
        "-   Bibliography\n",
        "-   Sources\n",
        "-   Citations\n",
        "-   etc.\n",
        "\n",
        "This left us with 4856 usable sections spread across 964 wikipedia pages.\n",
        "These sections were also slightly cleaned by removing whitespaces and Wikipedia specific markup elements from their articles.\n",
        "\n",
        "Each article was processed in so-called chunks of roughly 1600 tokens.\n",
        "These chunks are what will become our searchable and retrievable elements.\n",
        "If possible, chunks are made to be a single section of an article.\n",
        "In some cases, this was not possible and a single section is split into multiple chunks.\n",
        "For each chunk an corresponding embedding is generating using the API.\n",
        "we can compare the embeddings of our chunks, looking for the embedding or the embeddings that most closely resemble our question.\n",
        "Using this we can select the chunks that will be shared with our LLM during inference as the context that might answer our question.\n",
        "Of course, it might also be that there are no chunks that are relevant to our question.\n",
        "\n",
        "This data set can be found under: `./data/WIKIPEDIA_natural_language_processing.csv`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwslO3-sjTw"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "First we will need to install the necessary python packages.\n",
        "Luckily for us, google colab comes with most of the libraries and requiered cuda software already pre-installed.\n",
        "\n",
        "## 1.1 Runtime\n",
        "---\n",
        "\n",
        "We will want to use a GPU to run the examples in this notebook. In Google Colab, go to the menu bar:\n",
        "\n",
        "\n",
        "**Menu bar > Runtime > Change runtime type > T4 GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Install packages\n",
        "Run the cell below to install `llama-cpp-python` which allows fast inference on GPU and CPU with GGUF quantized models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% capture\n",
        "!pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!pip install adjustText\n",
        "!pip install umap-learn\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "iV7MV8m6vQR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb7GhDi9MHTW"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from umap import UMAP\n",
        "import re\n",
        "from fuzzywuzzy import fuzz\n",
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Matplotlib\n",
        "plt.style.use('seaborn-v0_8-darkgrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Load helper functions from github\n",
        "\n",
        "In the repo I have included 2 helper functions for talking to LLMs:\n",
        "\n",
        "1. generate_response\\\n",
        "   Generate a response given a set of chat messages, with optional streaming behavior.\n",
        "\n",
        "2. interactive_chat\\\n",
        "   Allows the user to engage in an interactive chat session with the model (streaming by default).\n"
      ],
      "metadata": {
        "id": "yBWj-Yy1H8OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o helper_functions.py https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/course_2025/exercises/day_1/helper_functions/helper_functions.py"
      ],
      "metadata": {
        "id": "WT87Yme-IFuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import *\n"
      ],
      "metadata": {
        "id": "WNsT3b7kIRGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0elsbJMMHTX"
      },
      "source": [
        "# Embedding model\n",
        "We will generate text-embeddings (just like the ones used inside LLMs) using [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct). This is a high quality embedding model, among one of the best model for retrieval embeddings available [Click here for a leaderboard with many benchmarks](https://huggingface.co/spaces/mteb/leaderboard).\n",
        "\n",
        "These embeddings are usefull for tasks such as:\n",
        "\n",
        "- Semantic Search\n",
        "- Semantic Clustering\n",
        "- Recommender Systems\n",
        "- Anomaly Detection\n",
        "- Classification (where text strings are classified by their most similar label)\n",
        "\n",
        "The maximum amount of tokens that can be embedded in a single `multilingual-e5-large-instruct` embedding is 512. Anything longer is truncated.\n",
        "\n",
        "Use the model to generate an embedding for a string of choice:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "H5BdCQuPPwLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")"
      ],
      "metadata": {
        "id": "0RL9EL0ma1fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = embeddings = embedding_model.encode(\" <- Your input here ->\")\n"
      ],
      "metadata": {
        "id": "_Wax1EGHRIFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the embedding to 3 random sentences below, which one is most similair to your sentence?"
      ],
      "metadata": {
        "id": "d8wRmKD1bJ1B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMxNoqmMHTX"
      },
      "source": [
        "sentences = [\n",
        "    \"The weather is lovely today, the sun is shining and the sky is blue.\",\n",
        "    \"Studying lineair algebra is not always easy, but it is very usefull.\",\n",
        "    \"Not too long ago, the Toyota Prius was the best-known hybrid vehicle on the market.\"\n",
        "]\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "\n",
        "similarities = embedding_model.similarity(embedding, embeddings)\n",
        "similarities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that these similarity scores are distributed between 0.7 and 1, regardless of what the inputs are. This is an artifact from the training process for these embeddings models. this will be discussed some more later"
      ],
      "metadata": {
        "id": "mQ1ckV4-cVUL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z06bZzczMHTX"
      },
      "source": [
        "## A question the LLM cannot answer\n",
        "\n",
        "During the Masterclass we talked about the famous Lawsuit where lawyers got into trouble because they used ChatGPT.\n",
        "They cited precedents that were hallucinated by ChatGPT. This event took place in May 2023.\n",
        "Our LLM is not aware of this precise event, so it cannot give a detailed answer to this question.\n",
        "Let us give it a try. Write a query asking ChatGPT what it knows about this case.\n",
        "\n",
        "-   Don't make your user prompt too short, as this will be detrimental to the content retrieval later on. Use at least 3 sentences.\n",
        "-   You don't have to supply any specifics, but paint the general picture of the story.\n",
        "-   Ask ChatGPT what ended up being the consequences for the lawyers in this court case.\n",
        "-   This is the user prompt we will use throughout this notebook, you will not change it after this point.\n",
        "-   **Instruct the LLM to only answer if it knows the answer. Otherwise is should reply with: \"I do not know the answer to this question\"**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Uncomment and run this cell if you need to clear the GPU memory!\n",
        "# import gc\n",
        "# import torch\n",
        "# del llm\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0DX_-DfGd7FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load you llm model\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384, # this is 25 A4 pages of context window!\n",
        "    verbose=False,\n",
        "    logits_all=True\n",
        ")"
      ],
      "metadata": {
        "id": "5pWoVCn3c8U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZNX5cMJMHTX"
      },
      "source": [
        "# Generate a classification\n",
        "system_prompt = \"\"\"\n",
        "<- Your input here ->\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "<- Your input here ->\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "response_generator = generate_response(llm, messages, stream=True, max_tokens=1024, temperature=0)\n",
        "for token in response_generator:\n",
        "    print(token, end='', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxeMmR1qMHTY"
      },
      "source": [
        "As you can see, the LLM does not know of what case we speak.\n",
        "We can be happy about 1 thing:\n",
        "\n",
        "- It told us that it has no knowledge of these events, and did not make up an answer instead.\n",
        "\n",
        "We are going to prove in the next few sections, that given the right technique, we can get the LLM to tells us about this unknown event anyways.\n",
        "\n",
        "## Retrieve the data, what does it look like?\n",
        "\n",
        "Load the data set and explore a little, what does the data look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ndKaJ8JMHTY"
      },
      "source": [
        "# load the dataset\n",
        "data = pd.read_csv(\"./data/WIKIPEDIA_natural_language_processing.csv\", sep=\",\")\n",
        "\n",
        "# We must parse the dataset into the right format for later.\n",
        "# Right now the embeddings have been loaded as strings,  convert them to vectors straight away.\n",
        "# Don't forget to remove the [ and ] from the string. If you get stuck, consider asking your LLM (or Gemini) for help\n",
        "def string_to_array(vector_string):\n",
        "       # Remove brackets and split the string into elements\n",
        "       elements = vector_string[1:-1].split(', ')\n",
        "       # Convert elements to floats and create a numpy array\n",
        "       return np.array(list(map(float, elements)))\n",
        "\n",
        "<- Your input here ->\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9duM7f2MHTY"
      },
      "source": [
        "data.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n23vdzkHMHTY"
      },
      "source": [
        "# Playing with embeddings\n",
        "\n",
        "In order to find the correct contextual information, we will need to embed our `user_prompt`, as it will be the key to our search algorithm.\n",
        "Go ahead and create an embedding of your user prompt, use the `generate_query` function to generate a query in the format that `intfloat/multilingual-e5-large-instruct` expects. Next use the `embedding_model` to create an embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxNujwqVMHTY"
      },
      "source": [
        "query = generate_query(user_prompt)\n",
        "prompt_embedding = embedding_model.encode(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZDJJnQtMHTZ"
      },
      "source": [
        "What are the number of dimensions of the embedding?\n",
        "and what is the length of this vector? This is calculated using the [Euclidean norm](https://mathworld.wolfram.com/L2-Norm.html)\n",
        "Round the result to 3 decimal places."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so8y2UT3MHTZ"
      },
      "source": [
        "<- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiqwM9rbMHTZ"
      },
      "source": [
        "With such a high number of dimensions, there is a lot of information about prompt that is encoded in this vector.\n",
        "These embeddings have been normalized. This means they have unit length.\n",
        "Because of this, these embeddings only have 1 fundamental quality that is informative to us and that is the pointing direction of the vector.\n",
        "This direction contains all the information the model has about the meaning of the embedded text:\n",
        "\n",
        "- Chunks with more similar meaning have more similar embeddings,\n",
        "- Chunks with more different meaning have more different embeddings.\n",
        "\n",
        "Play around with this property. Write the same sentence twice, worded slightly differently, how do these embeddings compare? You can use `embedding_model.similarity` do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMJBhAi0MHTZ"
      },
      "source": [
        "sentence_1 = \"<- Your input here ->\"\n",
        "sentence_2 = \"<- Your input here ->\"\n",
        "embedding_model.similarity(embedding_model.encode(sentence_1), embedding_model.encode(sentence_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L2p94KSMHTZ"
      },
      "source": [
        "Now write a third sentence with the same topic of the first sentence, but with the opposite polarity for example. How does this embedding compare?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFLpeaqJMHTZ"
      },
      "source": [
        "sentence_3 = \"<- Your input here ->\"\n",
        "print(\n",
        "    embedding_model.similarity(embedding_model.encode(sentence_1), embedding_model.encode(sentence_3)),\n",
        "    embedding_model.similarity(embedding_model.encode(sentence_2), embedding_model.encode(sentence_3))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM20m1GuMHTZ"
      },
      "source": [
        "Now write a fourth sentence that shares no similarities whatsover with the first sentence. How does this embedding compare?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrCf5ytGMHTZ"
      },
      "source": [
        "sentence_4 = \"<- Your input here ->\"\n",
        "print(\n",
        "    embedding_model.similarity(embedding_model.encode(sentence_1), embedding_model.encode(sentence_4)),\n",
        "    embedding_model.similarity(embedding_model.encode(sentence_2), embedding_model.encode(sentence_4)),\n",
        "    embedding_model.similarity(embedding_model.encode(sentence_3), embedding_model.encode(sentence_4))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPg6Er4oMHTZ"
      },
      "source": [
        "As you can see, even for topics that share no common subject the score does not go below ~ 0.7\n",
        "\n",
        "## Visualising our embedding space\n",
        "\n",
        "We can make some interesting visualizations from our embeddings. This will demonstrate that our embeddings capture a lot of semantic information about the texts that are embedded.\n",
        "Here we will project our high-dimensional embeddings onto 2 dimensions using the `umap` algorithm.\n",
        "These projections lose a lot of the original information, but allow us to plot our data.\n",
        "`UMAP` is a dimensionality reduction algorithm that works well at preserving both global and local structure in the data.\n",
        "\n",
        "Fill in the code in the cells below to achieve the following:\n",
        "\n",
        "- For each page (article), calculate the average embedding. Don't forget to normalize the length after averaging.\n",
        "- Calculate the reduced embedding form in 2 dimensions.\n",
        "- Sample these reduced form embeddings for N pages.\n",
        "- Plot the results, including page titles.\n",
        "\n",
        "The projection steps have been performed for you in this cells below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtBRgR0FMHTZ"
      },
      "source": [
        "# Get 1 average embedding per page\n",
        "data_average = data.groupby(\"page\").agg(\n",
        "    {\"embedding\": lambda x: np.mean(np.vstack(x), axis=0)}\n",
        ")\n",
        "data_average[\"embedding\"] = data_average[\"embedding\"].apply(\n",
        "    lambda x: x / np.linalg.norm(x, 2)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_iG-IPmMHTa"
      },
      "source": [
        "# Calculate reduced embeddings\n",
        "embedding_matrix = np.vstack(data_average['embedding'])\n",
        "umap_model = UMAP()\n",
        "umap_embeddings = umap_model.fit_transform(embedding_matrix)\n",
        "\n",
        "# Perform UMAP dimensionality reduction\n",
        "umap_model = UMAP()\n",
        "umap_embeddings = umap_model.fit_transform(embedding_matrix)\n",
        "\n",
        "# Add the UMAP embeddings to the data_average dataframe\n",
        "data_average['x1'] = umap_embeddings[:, 0]\n",
        "data_average['x2'] = umap_embeddings[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr9JI-BFMHTa"
      },
      "source": [
        "# Sample the reduced embeddings for N pages\n",
        "np.random.seed(1234)\n",
        "n_pages = <- Your input here ->\n",
        "data_plot = data_average.drop(columns='embedding').sample(n=n_pages).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCY0KdaMHTa"
      },
      "source": [
        "Visualize the different Wikipedia articles using the code below.\n",
        "It is best to play with the value for n_pages a bit, as well as the text size.\n",
        "Open the figure in full screen.\n",
        "Explain, what do you see?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an interactive scatter plot\n",
        "fig = px.scatter(\n",
        "    data_plot,\n",
        "    x=\"x1\",\n",
        "    y=\"x2\",\n",
        "    color=\"page\",        # Colors each point by its category\n",
        "    text=\"page\",         # Display the category as a text label on each point\n",
        "    title=\"Interactive Scatter Plot of Pages\",\n",
        "    labels={\n",
        "        \"x1\": \"X Axis Label\",\n",
        "        \"x2\": \"Y Axis Label\",\n",
        "        \"page\": \"Category\"\n",
        "    },\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "\n",
        "# Update the traces to adjust marker and text appearance\n",
        "fig.update_traces(\n",
        "    marker=dict(size=10, opacity=0.6),\n",
        "    textfont=dict(size=8, color='black'),\n",
        "    textposition='top center',  # Places the text just above each marker\n",
        "    selector=dict(mode='markers+text')  # Ensure both markers and text are updated\n",
        ")\n",
        "\n",
        "# Further update the layout for a more polished look\n",
        "fig.update_layout(\n",
        "    title=dict(\n",
        "        text=\"Interactive Scatter Plot of Pages\",\n",
        "        x=0.5,  # Center the title\n",
        "        xanchor='center',\n",
        "        font=dict(size=24)\n",
        "    ),\n",
        "    legend_title_text=\"Categories\",\n",
        "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, title=\"\"),\n",
        "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, title=\"\"),\n",
        "    width=1600,  # Adjust width (default is often too wide)\n",
        "    height=900  # Increase height for a taller view\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "v4rx8nIsofnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0BvcTecMHTa"
      },
      "source": [
        "## Performing similarity search using our embeddings - euclidean distance\n",
        "\n",
        "Now we want to start comparing our embeddings to one another.\n",
        "One might think of just calculating a regular distance metric like euclidean distance for our embeddings, after all our vectors represent a single point in space.\n",
        "So we could just look to see which ones are the closest together.\n",
        "However, in practice that will not work.\n",
        "*Can you think of a reason why not?*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <- Your input here ->"
      ],
      "metadata": {
        "id": "3tMZ88XxxbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing similarity search using our embeddings - cosine distance\n",
        "\n",
        "The easiest way to to compare our embeddings is through the metric called `cosine similarity`.\n",
        "Because our vectors are already normalised, we can use take the dot-product (inner product) between 2 vectors to directly calculate the angle between them, this is the cosine distance.\n",
        "You can calculate the cosine similarity by using `vector_1 @ vector_2`, or you can use `embedding_model.similarity`\n",
        "\n",
        "Calculate the cosine similarity between our prompt_embedding and all embeddings in the dataset, store this value in the column `similarity score`."
      ],
      "metadata": {
        "id": "gtRRSOFdxbVY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkH3luWQMHTa"
      },
      "source": [
        "# Apply the function row-wise\n",
        "calculate_similarity = lambda x: <- Your input here ->\n",
        "data['similarity_score'] = <- Your input here ->\n",
        "\n",
        "# Sort the DataFrame by similarity_score in descending order\n",
        "data_similarity = <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSF28PoiMHTa"
      },
      "source": [
        "Plot the distribution of similarity scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-dpi": 100,
        "fig-width": 18,
        "fig-height": 12,
        "id": "PYUPX8ZaMHTa"
      },
      "source": [
        "# Create the histogram\n",
        "bins = np.linspace(-1, 1, 101)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(data_similarity['similarity_score'], bins=bins, alpha=0.8, edgecolor='black')\n",
        "plt.xlim(-1, 1)\n",
        "plt.title(\"Distribution cosine similarity user prompt and Wikipedia embeddings\")\n",
        "plt.xlabel('Similarity Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaO-GN5CMHTa"
      },
      "source": [
        "It is somewhat suprising what you see in this plot. You'd expect there to be a very large range from -1 to 1:\n",
        "\n",
        "- If vectors are the same it is 1\n",
        "- If vectors are orthogonal it is 0\n",
        "- If vectors are in opposite directions it is -1\n",
        "\n",
        "but instead the values are concentrated around 0.7.\n",
        "What this essentially means is that all our vectors share a big component in the same direction\n",
        "\n",
        "To see what is going on, we will make another figure.\n",
        "Make another plot showing the distribution of cosine similarities with respect to the user prompt, together with the following 3 input prompts:\n",
        "\n",
        "-   label: \"Deltaplan\" - prompt:\"The estuaries of the rivers Rhine, Meuse and Schelde have been subject to flooding over the centuries. After building the Afsluitdijk (1927 – 1932), the Dutch started studying the damming of the Rhine-Meuse Delta. Plans were developed to shorten the coastline and turn the delta into a group of freshwater coastal lakes. By shortening the coastline, fewer dikes would have to be reinforced. Due to indecision and the Second World War, little action was taken. In 1950 two small estuary mouths, the Brielse Gat near Brielle and the Botlek near Vlaardingen were dammed. After the North Sea flood of 1953, a Delta Works Commission was installed to research the causes and develop measures to prevent such disasters in future. They revised some of the old plans and came up with the 'Deltaplan'.\"\n",
        "-   label: \"Baking\" - prompt:\"What is the best recipe for baking a carrot cake? I prefer them with not too many spices. The icing should be made with cream cheese of course\"\n",
        "-   label: \"Astronomy\" - prompt:\"A few years ago I remember seeing newspaper articles about astronomers imaging the black hole in our own milky way, how did they achieve this?\"\n",
        "\n",
        "Use these labels to add fill color to your histogram:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-dpi": 100,
        "fig-width": 18,
        "fig-height": 12,
        "id": "oMFfYj13MHTb"
      },
      "source": [
        "results = []\n",
        "prompts = [\n",
        "  \"The estuaries of the rivers Rhine, Meuse and Schelde have been subject to flooding over the centuries. After building the Afsluitdijk (1927 – 1932), the Dutch started studying the damming of the Rhine-Meuse Delta. Plans were developed to shorten the coastline and turn the delta into a group of freshwater coastal lakes. By shortening the coastline, fewer dikes would have to be reinforced. Due to indecision and the Second World War, little action was taken. In 1950 two small estuary mouths, the Brielse Gat near Brielle and the Botlek near Vlaardingen were dammed. After the North Sea flood of 1953, a Delta Works Commission was installed to research the causes and develop measures to prevent such disasters in future. They revised some of the old plans and came up with the 'Deltaplan'.\",\n",
        "  \"What is the best recipe for baking a carrot cake? I prefer them with not too many spices. The icing should be made with cream cheese of course\",\n",
        "  \"A few years ago I remember seeing newspaper articles about astronomers imaging the black hole in our own milky way, how did they achieve this?\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "  \"Deltaplan\",\n",
        "  \"Baking\",\n",
        "  \"Astronomy\"\n",
        "]\n",
        "\n",
        "# Add the embedding matrix to the results lists\n",
        "for i, prompt in enumerate(prompts):\n",
        "    embedding_tmp = embedding_model.encode(generate_query(prompt))\n",
        "    calculate_similarity = lambda x: embedding_model.similarity(embedding_tmp.astype(np.float64), x.embedding).item()\n",
        "    data['similarity_score'] = data.apply(calculate_similarity, axis=1)\n",
        "    data_similarity_tmp = data.sort_values(by='similarity_score', ascending=False)\n",
        "    data_similarity_tmp['label'] = labels[i]\n",
        "    results.append(data_similarity_tmp)\n",
        "\n",
        "data_similarity[\"label\"] = \"user_prompt\"\n",
        "results.append(data_similarity)\n",
        "\n",
        "# Combine the dataframes with pd.concat\n",
        "result = pd.concat(results, ignore_index=True)\n",
        "\n",
        "# plot the histogram of the 4 prompts at the same time\n",
        "# Set figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Define a list of colors for each histogram\n",
        "colors = ['blue', 'yellow', 'green', 'red']\n",
        "\n",
        "# Plot a histogram for each label\n",
        "for i, label in enumerate(result['label'].unique()):\n",
        "    subset = result[result['label'] == label]\n",
        "    plt.hist(subset['similarity_score'], bins=30, alpha=0.3, color=colors[i], label=label)\n",
        "\n",
        "# Customize plot aesthetics\n",
        "plt.xlabel(\"Similarity Score\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.title(\"Histogram of Similarity Scores for Different Prompts\", fontsize=16)\n",
        "plt.legend(title=\"Prompt Label\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2k20wrnMHTb"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "You can see the following:\n",
        "\n",
        "- The embeddings of `intfloat/multilingual-e5-large-instruct` have a  small dynamic range.\n",
        "\n",
        "- This in itself is not a big issue. We still see that the user prompt on AI got the highest similarity with the dataset, while the prompt on baking got the lowest similarity with the dataset.\n",
        "\n",
        "- This does help demonstrate that in practice it is difficult to set a reasonable cut-off for cosine similarity as one can do with other embedding models\n",
        "\n",
        "## Collect best matches\n",
        "\n",
        "Retrieve the 3 values with the highest similarity_score with our original user prompt, and store them in a table called `context`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw-kdEHGMHTb"
      },
      "source": [
        "context = <- Your input here ->"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXm-sd-1MHTb"
      },
      "source": [
        "# Retrieval Augmented Generation\n",
        "\n",
        "Now we have all the steps to perform retrieval augmented generation.\n",
        "Let's get going!\n",
        "\n",
        "## First try\n",
        "\n",
        "We've kept the user_prompt from the beginning, which we will not touch.\n",
        "Instead, change the system prompt to achieve the following:\n",
        "\n",
        "- The 3 best matching context chunks from data_similarity should be passed to the model as context.\n",
        "- It should only answer, if the context provides an answer.\n",
        "- The answer should only be based on the context, nothing else.\n",
        "- The answer should be as detailed as possible.\n",
        "- As we are passing the best matches as additional context for the answer, it is custom to add these to the system prompt. In a typical chat interface the user doesn't see the system prompt, thus this step is not revealed to the user.\n",
        "\n",
        "Use the system_prompt function below, and pass the context as variables in the f-string. This ensures that the string is filled with a flexible context!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_D495ANMHTb"
      },
      "source": [
        "def system_prompt(context):\n",
        "  return f\"\"\"\n",
        "<- Your input here ->\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt(context)},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "response_generator = generate_response(llm, messages, stream=True, max_tokens=1024, temperature=0)\n",
        "for token in response_generator:\n",
        "    print(token, end='', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJhN4sPOMHTb"
      },
      "source": [
        "Yeah!\n",
        "Now we are getting answers.\n",
        "However, we are not quite there yet.\n",
        "Now we have an answer, but how can we know it is true?\n",
        "It could still be making this up.\n",
        "For a single example we can of course fact-check the sources ourselves.\n",
        "But automation is the name of the game: we need to find a better solution!\n",
        "\n",
        "## Making responses (more) verifiable\n",
        "\n",
        "Once again, we are going to pass text content to our system prompt.\n",
        "This time, it will be very important that the model only answers using direct quotes from the context-chunks:\n",
        "\n",
        "-   It should only answer in the format: (\\`Wikipedia article: name\\`): \"quoted text\"\n",
        "\n",
        "-   It should only answer, if the context provides an answer.\n",
        "\n",
        "-   Quotes should only be direct, no paraphrasing.\n",
        "\n",
        "-   It is not allowed to use ellipses to shorten a quote.\n",
        "\n",
        "-   The answer should be as detailed as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9u6htHMHTb"
      },
      "source": [
        "def system_prompt(context):\n",
        "  return f\"\"\"\n",
        "<- Your input here ->\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt(context)},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "response_generator = generate_response(llm, messages, stream=True, max_tokens=1024, temperature=0)\n",
        "for token in response_generator:\n",
        "    print(token, end='', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfxpmHyyMHTc"
      },
      "source": [
        "Hooray!\n",
        "You can see that the model is mostly doing what we want.\n",
        "\n",
        "It is quoting directly from the context documents we retrieved with our simple RAG setup.\\\n",
        "It is giving some extra reasoning around these quotes,\n",
        "but that is not an issue.\n",
        "\n",
        "## Is our LLM quoting correctly?\n",
        "\n",
        "Now we want to verify the information provided by our LLM.\n",
        "We can use simple `regular expressions` to extract the sources from the response, as well as the quotes.\n",
        "We can check if these quotes are indeed present in the context we provided."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(llm, messages, max_tokens=1024, temperature=0)"
      ],
      "metadata": {
        "id": "xYFIXygv8RQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEGfo15mMHTc"
      },
      "source": [
        "# Regular Expression to capture the article name as it is in your context-table\n",
        "regex_articles = r\"<- Your input here ->\"\n",
        "\n",
        "# Using re.findall to extract all matches\n",
        "article_names = re.findall(regex_articles, response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_names"
      ],
      "metadata": {
        "id": "ziS5zDXv8xHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEYfdXeiMHTc"
      },
      "source": [
        "# Regular Expression to capture the content of the quote.\n",
        "regex_quotes = r\"<- Your input here ->\"\n",
        "\n",
        "# Using str_match_all to extract all matches\n",
        "quotes = re.findall(regex_quotes, response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quotes"
      ],
      "metadata": {
        "id": "Qa0DMQGp8vkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Wl5TZzMHTc"
      },
      "source": [
        "Now the question is, how do we check if these quotes are present in the text.\n",
        "We cannot use: `A in B` syntax.\n",
        "If our model changes a single character; it won't work to do literal matching.\n",
        "Instead, we will use a clever solution called Levenshtein distance.\n",
        "Please look up what Levenshtein distance is, what does it calculate?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <- Your input here ->"
      ],
      "metadata": {
        "id": "kFFMsTYw9gRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will make use of the function `fuzz.partial_ratio` from the fuzzywuzzy library.\n",
        "`partial_ratio` will look for the best location to match susbtring_x to a piece of string y, and then calculate the Levenshtein distance, and normalizes it by the length of the substring.\n",
        "For each quote found in the model answer above:\n",
        "\n",
        "- `correctness_quote`: Calculate the normalized Levensthein distance.\n",
        "- Make sure that you don't take into account the casing of the letters (upper or lower).\n",
        "- `correct_quote`: Check if the correctness value stays above 95%.\n",
        "\n",
        "If this is the case, correct_quote should read `TRUE` - It can happen that the same article (wikipedia page) is passed multiple times as context, but we take the excerpt that has the highest score."
      ],
      "metadata": {
        "id": "FIrNxAGo9hck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(fuzz.partial_ratio)"
      ],
      "metadata": {
        "id": "5qji7yw6_1AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFb5jD5_MHTc"
      },
      "source": [
        "for i, quote in enumerate(quotes, start=1):\n",
        "    # Filter context to only include rows for the current article\n",
        "    context_tmp = context[context['page'] == article_names[i-1]].copy()\n",
        "\n",
        "    # For each row, calculate the incorrectness percentage.\n",
        "    <- Your input here ->\n",
        "\n",
        "\n",
        "    # Flag the 'correct_quote' variable\n",
        "    <- Your input here ->\n",
        "\n",
        "\n",
        "    # Since the same article might appear multiple times, we pick the best match.\n",
        "    best_match_idx = context_tmp['correctness_quote'].idxmax()\n",
        "    best_match = context_tmp.loc[best_match_idx]\n",
        "\n",
        "    if not best_match['correct_quote']:\n",
        "        print(\n",
        "            f\"Quote {i} could not be attributed to the context sources, \"\n",
        "            f\"The quote had to be altered {best_match['correctness_quote']}% to fit the given article.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        literal_percentage = best_match['correctness_quote']\n",
        "        print(\n",
        "            f\"Quote {i} '{' '.join(quote.split(' ')[:5])} ... {' '.join(quote.split(' ')[-5:])}' is a correct quote from the article '{best_match['page']}, context document ', \"\n",
        "            f\"The quote was {literal_percentage}% literal from the article.\\n\"\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzlQNsWMHTc"
      },
      "source": [
        "## Bringing it together\n",
        "\n",
        "Now that we have a way of checking whether the quotes are not hallucinations, we can adjust our prompting function to do these calculations for us:\n",
        "\n",
        "- Fill out all the helper functions\n",
        "- Then fill in the main `RAG_completion` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjgBzUaSMHTc"
      },
      "source": [
        "# A helper function to isolate article sources from the model response\n",
        "def get_sources(answer):\n",
        "    <- Your input here ->\n",
        "\n",
        "\n",
        "# A helper function to isolate quotes from the model response\n",
        "def get_quotes(answer):\n",
        "    <- Your input here ->\n",
        "\n",
        "\n",
        "# A helper function to retrieve the relevant context\n",
        "def get_context(prompt_embedding):\n",
        "    <- Your input here ->\n",
        "\n",
        "\n",
        "# A helper function to check if the quotes are reasonable given the context\n",
        "def check_quotes(context, sources, quotes):\n",
        "    <- Your input here ->\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzb9lhOwMHTc"
      },
      "source": [
        "# Our RAG_completion main function\n",
        "def RAG_completion(system = system_prompt,\n",
        "                   prompt = user_prompt,\n",
        "                   temperature = 0):\n",
        "    # Generate the embedding for our prompt\n",
        "    query = generate_query(prompt)\n",
        "    prompt_embedding = embedding_model.encode(query)\n",
        "\n",
        "    # Retrieve the 3 most relevant context chunks in a df called context\n",
        "    context = get_context(prompt_embedding)\n",
        "\n",
        "    # Generate our model response\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": system(context)},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    response = generate_response(llm, messages, max_tokens=1024, temperature=0)\n",
        "    print(response)\n",
        "\n",
        "    # extract the sources and quotes\n",
        "    sources = get_sources(response)\n",
        "    quotes = get_quotes(response)\n",
        "\n",
        "    # Verify the model quotes. Raise a warning if a quote does not reach the standard\n",
        "    print(\"\\n\\n=============== Verifying Answer =================\\n\\n\")\n",
        "    check_quotes(context, sources, quotes)\n",
        "\n",
        "    return(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrPGQ4flMHTd"
      },
      "source": [
        "response = RAG_completion(system_prompt, user_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwIMYfNtMHTd"
      },
      "source": [
        "## Congratulations, try it out for other questions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUlz5LaRMHTd"
      },
      "source": [
        "user_prompt = \"\"\"\n",
        "Who is Andrew NG? Why do I see him a lot on youtube?\n",
        "\"\"\"\n",
        "\n",
        "response = RAG_completion(system=system_prompt, prompt=user_prompt, temperature=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KbaZDqXMHTd"
      },
      "source": [
        "## (Optional) It's LLMs all the way down.\n",
        "\n",
        "What if you pass the output answer from `RAG_completion` to a new chat, along with the context.\n",
        "Can you get another chat with the LLM to verify whether the answer is correct given the content and the quote checks?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "openai",
      "language": "python",
      "display_name": "openai-chatgpt"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}