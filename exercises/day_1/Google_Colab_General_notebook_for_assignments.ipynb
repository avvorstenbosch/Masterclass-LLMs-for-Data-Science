{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "# General notebook for assignments\n",
        "---\n",
        "\n",
        "Part of the [Masterclass: Large Language Models for Data Science](https://github.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science)\n",
        "![](https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/main/slides/day_1/figures/pair-programming-with-llms.webp)\n",
        "by Alex van Vorstenbosch\n",
        "---\n",
        "\n",
        "This is a general notebook for making assignments, which you can use to interact with open source LLM models instead of closed source models via API's.\n",
        "\n",
        "Google Colab provides both CPU and GPU compute resources. For the free teir users get access to a Nvidia T4 (2018) which has:\n",
        "* 16 [GiB](https://www.techtarget.com/searchstorage/definition/gibibyte-GiB) VRAM\n",
        "* 2560 CUDA cores\n",
        "\n",
        "Access is limited in time, but overall these limits are very gracious to users.\n",
        "If required, upgrading to a subscription will provide more compute credits\n",
        "\n",
        "You can run open source LLM models of up to **~7B parameters**. The standard these days is that LLMs are run in half-precision (FP16: 2 bytes/8 bits per parameter) as compared to full-precision FP32. This means a 7B model uses 14GB of VRAM. But you need to take into account the context window you are using. VRAM-usage scales linearly with the size of the context window. For a **~7B**, the context window will be limited to **~8192 tokens** (Due to the [KV-cache](https://arxiv.org/pdf/2412.19442))\n",
        "\n",
        "Luckily, researchers figured out that we can quantize the model weights to even smaller sizes such as Q4 (4 bits per parameter) while retaining much of the original model performance. Going beyond Q4 is typically not recommended. Quantization allows us to use models beyond 7B, but introduces a trade-off between speed/memory-usage and quality. As a rule-of-thumb: for performance it is better to use a bigger quantised model, than a smaller model without quantization. For online use I would always recommend using a quant, even just to save overhead in download times. Q6 only has very little degredation in quality, but saves a factor 2.67 in size.\n",
        "\n",
        "Here are a few models you can try on colab. Please note that new models are coming out every other day, a few suggestions might be outdated when you read this!\n",
        "\n",
        "**Some model suggestions:**\n",
        "* tiny: 3.8B - [Microsoft Phi-4-mini quantized](https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF)\n",
        "\n",
        "* medium: 8B - [Google Gemma-3 12B-Instruction Tuned quantized](https://huggingface.co/unsloth/gemma-3-12b-it-GGUF)\n",
        "\n",
        "* Large: 24B - [Mistral small-3.2-24B-2506 quantized](https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF)\n",
        "\n",
        "* SFT reasoning - [Deepseek R1-0528-Qwen-8B quantized](https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwslO3-sjTw"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "First we will need to install the necessary python packages.\n",
        "Luckily for us, google colab comes with most of the libraries and requiered cuda software already pre-installed.\n",
        "\n",
        "## 1.1 Runtime\n",
        "---\n",
        "\n",
        "We will want to use a GPU to run the examples in this notebook. In Google Colab, go to the menu bar:\n",
        "\n",
        "\n",
        "**Menu bar > Runtime > Change runtime type > T4 GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Install packages\n",
        "Run the cell below to install `llama-cpp-python` which allows fast inference on GPU and CPU with GGUF quantized models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7MV8m6vQR-",
        "outputId": "52927555-f64b-43bc-9680-f89bcba9b572"
      },
      "outputs": [],
      "source": [
        "# %% capture\n",
        "!pip install --no-cache-dir llama-cpp-python==0.3.16 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF52q2f89I81"
      },
      "source": [
        "## (Optional) 1.3 Clear GPU VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQPP3TvG9FPD"
      },
      "outputs": [],
      "source": [
        "# ## Uncomment and run this cell if you need to clear the GPU memory!\n",
        "# import gc\n",
        "# import torch\n",
        "\n",
        "# # Sometimes you need a second run of this cell to clear the memory, in that case you need to comment out this line\n",
        "# del llm\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBWj-Yy1H8OI"
      },
      "source": [
        "## 1.4 Load helper functions from github\n",
        "\n",
        "In the repo I have included 2 helper functions for talking to LLMs:\n",
        "\n",
        "1. generate_response\\\n",
        "   Generate a response given a set of chat messages, with optional streaming behavior.\n",
        "\n",
        "2. interactive_chat\\\n",
        "   Allows the user to engage in an interactive chat session with the model (streaming by default).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT87Yme-IFuc",
        "outputId": "a03a764d-6652-4449-f4d1-ebba785450b6"
      },
      "outputs": [],
      "source": [
        "!curl -o helper_functions.py https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/main/exercises/day_1/helper_functions/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNsT3b7kIRGg"
      },
      "outputs": [],
      "source": [
        "from helper_functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXGpozg3Ixga",
        "outputId": "f8b767d5-fa2d-4478-c6dc-5fd2c63e7700"
      },
      "outputs": [],
      "source": [
        "help(generate_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coznZx7mI6cs",
        "outputId": "cabd2846-049a-497c-bd8a-1d8e28fc9170"
      },
      "outputs": [],
      "source": [
        "help(interactive_chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaB0B1LdMcPM"
      },
      "source": [
        "# 2 Loading our model\n",
        "\n",
        "## 2.1 Setting your Huggingface token as a Colab Secret\n",
        "\n",
        "Some repositories on `huggingface ðŸ¤—` are gated, which means you need to request access to be able to download the models. In order to access these models via code, make sure to add the `HF_TOKEN` to your colab secrets.\n",
        "\n",
        "To find a quick guide for how to do this, [click here](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75).\n",
        "\n",
        "If no `HF_TOKEN` is set you will receive a warning, also when downloading models without gated access, but you can ignore it without any issues.\n",
        "\n",
        "\n",
        "## 2.2 Model selection and download\n",
        "\n",
        "Select a model on huggingface of your chosing in the `Llama.from_pretrained` function below.\n",
        "\n",
        "By default we use the `Google Gemma 3 12B GGUF` model, as this strikes a nice balance between quality and speed.\n",
        "\n",
        "In case you want a smaller and faster model, you can select `Windows Phi 4` using:\n",
        "\n",
        "```\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"unsloth/gemma-3-12b-it-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=32518, # this is 50 A4 pages of context window!\n",
        "    verbose=False\n",
        ")\n",
        "```\n",
        "If instead you want to use one of the most powerfull models currently available, consider using `Mistral-Small-3.2-24B-Instruct-2506-GGUF`:\n",
        "\n",
        "```\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q4_0.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096, # this is 6 A4 pages of context window\n",
        "    verbose=False\n",
        ")\n",
        "```\n",
        "\n",
        "As a default this notebook a medium size Google model called `gemma-3-12b-it-GGUF`. Please note that this download will take anywhere between 2 and 8 minutes to download.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "dc260d9a3b014886a5cd9742b5bbe7c9",
            "d4a8a2752249477fb391564ddfe4b9d0",
            "e62d1e2be1734aa29f7b491f4480b6a5",
            "49d2f8a06cea4029ada35ba4f65adf46",
            "97dcb43e5c5640899f76b4d74c25e4d0",
            "7de0610f7ff94757abebc822f2578cd5",
            "f6f5f5e0b26b4698b42ab534d3abf02c",
            "2adb75afe348433a9e15ca44d95815a9",
            "b9e0a38b1e764193addd01b6ff44f655",
            "8a9058c51fc147338a87741bfe9c0853",
            "cfb788318ed841168c83ff8e95d640ea"
          ]
        },
        "id": "mNkbw28oMeAM",
        "outputId": "30d89191-505a-4bac-cad1-851639e3541b"
      },
      "outputs": [],
      "source": [
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Load you llm model\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"unsloth/gemma-3-12b-it-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=32518, # this is 50 A4 pages of context window!\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UYlINgBLXlS"
      },
      "source": [
        "# 3 Running models\n",
        "\n",
        "## 3.1 Non-interactive responses\n",
        "Best for tasks that only require a single response, no back and forth interaction.\\\n",
        "i.e. generating summaries, translations, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kxf4Xexhtel_",
        "outputId": "e7a025e3-1130-4f00-fe35-9e975b1ccd8a"
      },
      "outputs": [],
      "source": [
        "# Define the conversation history\n",
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# specify the system message\n",
        "system_role = \"\"\"\n",
        "  You are a helpfull assistant. Your task is to write short rymes about the user input topic.\n",
        "\"\"\"\n",
        "\n",
        "# Provide your specific input\n",
        "user_question = \"\"\"\n",
        "  A cat sleeping on the computer keyboard is kneading with its paws, accidentally talking to an llm who is responding to the random keystrokes of the kitten.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_role},\n",
        "    {\"role\": \"user\", \"content\": user_question}\n",
        "]\n",
        "\n",
        "generate_response(llm, messages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ODC4O53C_mq"
      },
      "source": [
        "# 3.2 Interactive mode\n",
        "Use this if you want to have a functional chat with an LLM.\n",
        "A very basic 'chatgpt' interface reading your input from the keyboard, and printing responses via streaming.\n",
        "\n",
        "*Type `exit` to leave the chat*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHyvtEKh_z4y",
        "outputId": "301937e4-8e0f-46cc-f573-8c34a1e5485e"
      },
      "outputs": [],
      "source": [
        "interactive_chat(llm, system_prompt=\"You are chad gippity, a helpfull assistant.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2adb75afe348433a9e15ca44d95815a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d2f8a06cea4029ada35ba4f65adf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9058c51fc147338a87741bfe9c0853",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cfb788318ed841168c83ff8e95d640ea",
            "value": "â€‡6.60G/6.60Gâ€‡[02:36&lt;00:00,â€‡42.8MB/s]"
          }
        },
        "7de0610f7ff94757abebc822f2578cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9058c51fc147338a87741bfe9c0853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97dcb43e5c5640899f76b4d74c25e4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e0a38b1e764193addd01b6ff44f655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfb788318ed841168c83ff8e95d640ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a8a2752249477fb391564ddfe4b9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de0610f7ff94757abebc822f2578cd5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f6f5f5e0b26b4698b42ab534d3abf02c",
            "value": "Meta-Llama-3.1-8B-Instruct-Q6_K.gguf:â€‡100%"
          }
        },
        "dc260d9a3b014886a5cd9742b5bbe7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4a8a2752249477fb391564ddfe4b9d0",
              "IPY_MODEL_e62d1e2be1734aa29f7b491f4480b6a5",
              "IPY_MODEL_49d2f8a06cea4029ada35ba4f65adf46"
            ],
            "layout": "IPY_MODEL_97dcb43e5c5640899f76b4d74c25e4d0"
          }
        },
        "e62d1e2be1734aa29f7b491f4480b6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2adb75afe348433a9e15ca44d95815a9",
            "max": 6596011424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9e0a38b1e764193addd01b6ff44f655",
            "value": 6596011424
          }
        },
        "f6f5f5e0b26b4698b42ab534d3abf02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
