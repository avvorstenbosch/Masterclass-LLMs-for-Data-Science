{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "# General notebook for assignments\n",
        "---\n",
        "\n",
        "Part of the [Masterclass: Large Language Models for Data Science](https://github.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science)\n",
        "![](https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/main/slides/day_1/figures/pair-programming-with-llms.webp)\n",
        "by Alex van Vorstenbosch\n",
        "---\n",
        "\n",
        "This is a general notebook for making assignments, which you can use to interact with open source LLM models instead of closed source models via API's.\n",
        "\n",
        "Google Colab provides both CPU and GPU compute resources. For the free teir users get access to a Nvidia T4 (2018) which has:\n",
        "* 16 [GiB](https://www.techtarget.com/searchstorage/definition/gibibyte-GiB) VRAM\n",
        "* 2560 CUDA cores\n",
        "\n",
        "Access is limited in time, but overall these limits are very gracious to users.\n",
        "If required, upgrading to a subscription will provide more compute credits\n",
        "\n",
        "You can run open source LLM models of up to **~7B parameters**. The standard these days is that LLMs are run in half-precision (FP16: 2 bytes/8 bits per parameter) as compared to full-precision FP32. This means a 7B model uses 14GB of VRAM. But you need to take into account the context window you are using. VRAM-usage scales linearly with the size of the context window. For a **~7B**, the context window will be limited to **~8192 tokens** (Due to the [KV-cache](https://arxiv.org/pdf/2412.19442))\n",
        "\n",
        "Luckily, researchers figured out that we can quantize the model weights to even smaller sizes such as Q4 (4 bits per parameter) while retaining much of the original model performance. Going beyond Q4 is typically not recommended. Quantization allows us to use models beyond 7B, but introduces a trade-off between speed/memory-usage and quality. As a rule-of-thumb: for performance it is better to use a bigger quantised model, than a smaller model without quantization. For online use I would always recommend using a quant, even just to save overhead in download times. Q6 only has very little degredation in quality, but saves a factor 2.67 in size.\n",
        "\n",
        "Here are a few models you can try on colab. Please note that new models are coming out every other day, a few suggestions might be outdated when you read this!\n",
        "\n",
        "**Some model suggestions:**\n",
        "* tiny: 3.8B - [Microsoft Phi-3.5 quantized](https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF)\n",
        "\n",
        "* medium: 8B - [Meta Llama-3.1 8B-Instruct quantized](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF)\n",
        "\n",
        "* Large: 24B - [Mistral small-24B-2501 quantized](https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF)\\\n",
        "\n",
        "* SFT reasoning - [Deepseek R1-Distill-Qwen-14B quantized](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwslO3-sjTw"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "First we will need to install the necessary python packages.\n",
        "Luckily for us, google colab comes with most of the libraries and requiered cuda software already pre-installed.\n",
        "\n",
        "## 1.1 Runtime\n",
        "---\n",
        "\n",
        "We will want to use a GPU to run the examples in this notebook. In Google Colab, go to the menu bar:\n",
        "\n",
        "\n",
        "**Menu bar > Runtime > Change runtime type > T4 GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Install packages\n",
        "Run the cell below to install `llama-cpp-python` which allows fast inference on GPU and CPU with GGUF quantized models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7MV8m6vQR-",
        "outputId": "52927555-f64b-43bc-9680-f89bcba9b572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.2.90\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu122/llama_cpp_python-0.2.90-cp311-cp311-linux_x86_64.whl (443.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 MB\u001b[0m \u001b[31m255.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.90)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.90) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m236.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n"
          ]
        }
      ],
      "source": [
        "# %% capture\n",
        "!pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF52q2f89I81"
      },
      "source": [
        "## (Optional) 1.3 Clear GPU VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQPP3TvG9FPD"
      },
      "outputs": [],
      "source": [
        "### Uncomment and run this cell if you need to clear the GPU memory!\n",
        "# import gc\n",
        "# import torch\n",
        "# del llm\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBWj-Yy1H8OI"
      },
      "source": [
        "## 1.4 Load helper functions from github\n",
        "\n",
        "In the repo I have included 2 helper functions for talking to LLMs:\n",
        "\n",
        "1. generate_response\\\n",
        "   Generate a response given a set of chat messages, with optional streaming behavior.\n",
        "\n",
        "2. interactive_chat\\\n",
        "   Allows the user to engage in an interactive chat session with the model (streaming by default).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT87Yme-IFuc",
        "outputId": "a03a764d-6652-4449-f4d1-ebba785450b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7391  100  7391    0     0  12537      0 --:--:-- --:--:-- --:--:-- 12527\n"
          ]
        }
      ],
      "source": [
        "!curl -o helper_functions.py https://raw.githubusercontent.com/avvorstenbosch/Masterclass-LLMs-for-Data-Science/refs/heads/course_2025/exercises/day_1/helper_functions/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WNsT3b7kIRGg"
      },
      "outputs": [],
      "source": [
        "from helper_functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXGpozg3Ixga",
        "outputId": "f8b767d5-fa2d-4478-c6dc-5fd2c63e7700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function generate_response in module helper_functions:\n",
            "\n",
            "generate_response(llm, messages, max_tokens=128, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, json_mode=False, stream=False, **kwargs)\n",
            "    Generate a response from a Llama_CPP model given a list of messages.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    llm : Llama_cpp\n",
            "        The Llama_cpp llm model to use for generation.\n",
            "    messages : list of dict\n",
            "        A list of message dictionaries, where each dictionary should have\n",
            "        the keys {\"role\", \"content\"}. Example:\n",
            "        [\n",
            "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
            "            {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
            "        ]\n",
            "    max_tokens : int, optional\n",
            "        Maximum number of tokens to generate (default: 128).\n",
            "    temperature : float, optional\n",
            "        Sampling temperature. Higher values produce more random outputs\n",
            "        (default: 0.8).\n",
            "    top_p : float, optional\n",
            "        Nucleus sampling probability threshold (default: 0.95).\n",
            "    top_k : int, optional\n",
            "        The top-k tokens to consider during sampling (default: 40).\n",
            "    repeat_penalty : float, optional\n",
            "        Penalty for repeated tokens (default: 1.1).\n",
            "    json_mode : bool, optional\n",
            "        If True, response are formatted as JSON objects by the llm (default: False).\n",
            "    stream : bool, optional\n",
            "        If True, yield partial responses (tokens) as they are generated.\n",
            "        If False, return the full generated text at once (default: False).\n",
            "    **kwargs : dict\n",
            "        Additional keyword arguments passed to `create_chat_completion`.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    str or generator\n",
            "        If stream is False, returns the full response text as a string.\n",
            "        If stream is True, returns a generator that yields partial text chunks.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(generate_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coznZx7mI6cs",
        "outputId": "cabd2846-049a-497c-bd8a-1d8e28fc9170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function interactive_chat in module helper_functions:\n",
            "\n",
            "interactive_chat(llm, system_prompt='You are a helpful assistant.', max_tokens=512, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, stop_str='exit', **kwargs)\n",
            "    Start an interactive chat session with the Llama model in the console.\n",
            "    The model's responses are streamed in real time.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    llm : Llama\n",
            "        The Llama instance to use for the interactive chat.\n",
            "    system_prompt : str, optional\n",
            "        The system prompt that sets the context or persona of the assistant\n",
            "        (default: \"You are a helpful assistant.\").\n",
            "    max_tokens : int, optional\n",
            "        Maximum number of tokens to generate in each response (default: 512).\n",
            "    temperature : float, optional\n",
            "        Sampling temperature. Higher values produce more random outputs\n",
            "        (default: 0.8).\n",
            "    top_p : float, optional\n",
            "        Nucleus sampling probability threshold (default: 0.95).\n",
            "    top_k : int, optional\n",
            "        The top-k tokens to consider during sampling (default: 40).\n",
            "    repeat_penalty : float, optional\n",
            "        Penalty for repeated tokens (default: 1.1).\n",
            "    stop_str : str, optional\n",
            "        Keyword to terminate the conversation (default: \"exit\").\n",
            "    **kwargs : dict\n",
            "        Additional keyword arguments passed to `create_chat_completion`.\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    - Type the stop_str (default: \"exit\") on a line by itself to end the chat.\n",
            "    - User input is now single-line (press Enter once after typing).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(interactive_chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaB0B1LdMcPM"
      },
      "source": [
        "# 2 Loading our model\n",
        "\n",
        "## 2.1 Setting your Huggingface token as a Colab Secret\n",
        "\n",
        "Some repositories on `huggingface 🤗` are gated, which means you need to request access to be able to download the models. In order to access these models via code, make sure to add the `HF_TOKEN` to your colab secrets.\n",
        "\n",
        "To find a quick guide for how to do this, [click here](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75).\n",
        "\n",
        "If no `HF_TOKEN` is set you will receive a warning, also when downloading models without gated access, but you can ignore it without any issues.\n",
        "\n",
        "\n",
        "## 2.2 Model selection and download\n",
        "\n",
        "Select a model on huggingface of your chosing in the `Llama.from_pretrained` function below.\n",
        "\n",
        "By default we use the `Meta-Llama-3.1-B Q6 quant`, as this strikes a nice balance between quality and speed.\n",
        "\n",
        "In case you want a smaller and faster model, you can select `Windows Phi 3.5` using:\n",
        "\n",
        "```\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"bartowski/Phi-3.5-mini-instruct-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=65536, # this is 100 A4 pages of context window!\n",
        "    verbose=False\n",
        ")\n",
        "```\n",
        "If instead you want to use one of the most powerfull models currently available, consider using `Mistral-Small-24B-Instruct-2501-GGUF`:\n",
        "\n",
        "```\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"bartowski/Phi-3.5-mini-instruct-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q4_0.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096, # this is 6 A4 pages of context window\n",
        "    verbose=False\n",
        ")\n",
        "```\n",
        "\n",
        "As a default this notebook a medium size Meta model called `Meta-Llama-3.1-8B`. Please note that this download will take anywhere between 2 and 8 minutes to download.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "dc260d9a3b014886a5cd9742b5bbe7c9",
            "d4a8a2752249477fb391564ddfe4b9d0",
            "e62d1e2be1734aa29f7b491f4480b6a5",
            "49d2f8a06cea4029ada35ba4f65adf46",
            "97dcb43e5c5640899f76b4d74c25e4d0",
            "7de0610f7ff94757abebc822f2578cd5",
            "f6f5f5e0b26b4698b42ab534d3abf02c",
            "2adb75afe348433a9e15ca44d95815a9",
            "b9e0a38b1e764193addd01b6ff44f655",
            "8a9058c51fc147338a87741bfe9c0853",
            "cfb788318ed841168c83ff8e95d640ea"
          ]
        },
        "id": "mNkbw28oMeAM",
        "outputId": "30d89191-505a-4bac-cad1-851639e3541b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc260d9a3b014886a5cd9742b5bbe7c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Meta-Llama-3.1-8B-Instruct-Q6_K.gguf:   0%|          | 0.00/6.60G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Load you llm model\n",
        "llm = Llama.from_pretrained(\n",
        "    # Huggingface repo name\n",
        "    repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
        "    # select the quant file within the repo you want '*' is a wildcard selector\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=32518, # this is 50 A4 pages of context window!\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UYlINgBLXlS"
      },
      "source": [
        "# 3 Running models\n",
        "\n",
        "## 3.1 Non-interactive responses\n",
        "Best for tasks that only require a single response, no back and forth interaction.\\\n",
        "i.e. generating summaries, translations, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kxf4Xexhtel_",
        "outputId": "e7a025e3-1130-4f00-fe35-9e975b1ccd8a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"In silicon halls, a sight so rare,\\nA kitty slept, with paws in air.\\nKneading keys, with gentle touch,\\nA conversation started, by random clutch.\\n\\nThe LLn's response, came swift and bright,\\nTo 'meow', 'mew' and 'purrs', in digital light.\\nIt asked the cat, of its feline delight,\\nBut got a jumbled mix, of keyboard fright.\\n\\nThe kitten purred, as keys did clack,\\nAnd the LLn typed on, with digital track.\\nA strange dialogue, for all to see,\\nBetween a cat and AI, wild and carefree\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the conversation history\n",
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# specify the system message\n",
        "system_role = \"\"\"\n",
        "  You are a helpfull assistant. Your task is to write short rymes about the user input topic.\n",
        "\"\"\"\n",
        "\n",
        "# Provide your specific input\n",
        "user_question = \"\"\"\n",
        "  A cat sleeping on the computer keyboard is kneading with its paws, accidentally talking to an llm who is responding to the random keystrokes of the kitten.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_role},\n",
        "    {\"role\": \"user\", \"content\": user_question}\n",
        "]\n",
        "\n",
        "generate_response(llm, messages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ODC4O53C_mq"
      },
      "source": [
        "# 3.2 Interactive mode\n",
        "Use this if you want to have a functional chat with an LLM.\n",
        "A very basic 'chatgpt' interface reading your input from the keyboard, and printing responses via streaming.\n",
        "\n",
        "*Type `exit` to leave the chat*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHyvtEKh_z4y",
        "outputId": "301937e4-8e0f-46cc-f573-8c34a1e5485e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Interactive Chat ===\n",
            "Type 'exit' (without quotes) to exit.\n",
            "\n",
            "🧑‍💻 user: hello, who are you?\n",
            "✨ llm: Hello! I'm Chad Gippity, your friendly and helpful assistant. It's nice to meet you. I'm here to assist with any questions or topics you'd like to discuss. What brings you here today? Would you like some advice, information on a specific subject, or maybe just have a chat? Let me know how I can help!\n",
            "──────────────────────────────────────────────────\n",
            "🧑‍💻 user: exit\n",
            "\n",
            "🚪 Exiting chat.\n"
          ]
        }
      ],
      "source": [
        "interactive_chat(llm, system_prompt=\"You are chad gippity, a helpfull assistant.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2adb75afe348433a9e15ca44d95815a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d2f8a06cea4029ada35ba4f65adf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9058c51fc147338a87741bfe9c0853",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb788318ed841168c83ff8e95d640ea",
            "value": " 6.60G/6.60G [02:36&lt;00:00, 42.8MB/s]"
          }
        },
        "7de0610f7ff94757abebc822f2578cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9058c51fc147338a87741bfe9c0853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97dcb43e5c5640899f76b4d74c25e4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e0a38b1e764193addd01b6ff44f655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfb788318ed841168c83ff8e95d640ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a8a2752249477fb391564ddfe4b9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de0610f7ff94757abebc822f2578cd5",
            "placeholder": "​",
            "style": "IPY_MODEL_f6f5f5e0b26b4698b42ab534d3abf02c",
            "value": "Meta-Llama-3.1-8B-Instruct-Q6_K.gguf: 100%"
          }
        },
        "dc260d9a3b014886a5cd9742b5bbe7c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4a8a2752249477fb391564ddfe4b9d0",
              "IPY_MODEL_e62d1e2be1734aa29f7b491f4480b6a5",
              "IPY_MODEL_49d2f8a06cea4029ada35ba4f65adf46"
            ],
            "layout": "IPY_MODEL_97dcb43e5c5640899f76b4d74c25e4d0"
          }
        },
        "e62d1e2be1734aa29f7b491f4480b6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2adb75afe348433a9e15ca44d95815a9",
            "max": 6596011424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9e0a38b1e764193addd01b6ff44f655",
            "value": 6596011424
          }
        },
        "f6f5f5e0b26b4698b42ab534d3abf02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
