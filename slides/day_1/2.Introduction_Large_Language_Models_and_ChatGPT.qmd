---
title: "Introduction to Large Language Models, ChatGPT, and Generative AI" 
subtitle: ""
author: "Alex van Vorstenbosch"
footer: "LLMs and ChatGPT"
title-slide-attributes:
    data-background-image: "./figures/LLMs-and-chatgpt.png"
    data-background-opacity: "0.5"
---

## Hello world!

- <span class="highlighted-text">Generative AI</span>, is the biggest data-driven hype of the past years.

![](./figures/Gif-ChatGPT.gif){fig-align="center"}


# What is ChatGPT?
:::: {.columns}

::: {.column width="60%"}
- A Large Language model trained on enormous amounts of data:
    - Which was instruction tuned
    - Which was finetuned with RLHF
- Which resulted in:
    - *Extremely* impressive Chatbot capabilities
    - *Much* better interaction and
    - *Much* better language understanding while also 
    - *Accessible* to use for <span class="highlighted-text">Anybody</span>
:::
::: {.column width="40%"}
<iframe src="./webpages/ChatGPT for Data Science.html" class="iframe-chatgpt"></iframe>
:::
:::

## What is generative AI

- Generative AI:

::: {.center}

::: {.fragment}
<div style="text-align: center;">
*"In three words: deep learning worked."*
</div>
:::

::: {.fragment}
<div style="text-align: center;">
*"In 15 words: deep learning worked, got predictably better with scale, and we dedicated increasing resources to it."*
</div>
:::

::: {.fragment}
<div style="text-align: center;">
*"That’s really it; humanity discovered an algorithm that could really, truly learn any distribution of data (or really, the underlying “rules” that produce any distribution of data). To a shocking degree of precision, the more compute and data available, the better it gets at helping people solve hard problems. I find that no matter how much time I spend thinking about this, I can never really internalize how consequential it is."*
</div>
<div style="text-align: right;">
*- Sam Altman, [The Intelligence Age, 23-09-2024]*
</div>
:::

:::


## What is generative AI

- Generative AI:

::: {style="text-align: center"}
*"\
Artificial Intelligence that has learned to create data such as:\
images, text, audio, videos, etc...\
"*
:::

    - Text (Large Language Models)
    - Images (and Video) (Diffusion Models)
    - Audio (Text-to-Speech)
    - Tabular Data (Synthetic data)

- Typically meant in the context of content-creation

- This is **not new**: [thispersondoesnotexist.com (2018)](https://thispersondoesnotexist.com/)
    - For more check out [ThisXdoesnotexist.com](https://thisxdoesnotexist.com/)

- But became significantly more **powerfull**, **flexible**, **practical**, and **"mainstream"** around ~2022

## What is generative AI - Language Models

- Large Language Models are:
    - The State-of-the-Art for <span class ="highlighted-text" >generating language</span>
    - Famous LLMs are:
        - OpenAI: ChatGPT (GPT 3.5), GPT4(o) **(proprietary)**
        - Meta: LLama
        - Google: Gemini (Bard) **(proprietary)**
        - Alibaba: Qwen 
        - Anthropic: Claude **(proprietary)**
        - Mistral: Mistral, Mixtral 
        - Microsoft: Phi  

## What is generative AI - Image models

- Diffusion models are the State-of-the-Art for <span class ="highlighted-text" >generating images</span>:
    - Famous diffusion models are:
        - OpenAI: Dall-E **(proprietary)**
        - Midjourney: Midjourney **(proprietary)**
        - Stability-AI: Stable Diffusion

## Overview 
:::{.nonincremental}
- A brief history of LLMs
- Capabilities of ChatGPT
:::

## Overview 
:::{.nonincremental}
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
:::

# A brief history of LLMs
## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

A token: *a single character, a combination of characters, or a word*

![](./figures/what-are-tokens.png){fig-align="center"}

## Next-word prediction machine
$$P(token_n|token_{n-1}, \cdots, token_1)$$

This is nothing new, your phone does something similair: 

![](./figures/text-prediction-whatsapp.png){fig-align="center"}

## {.full-bg-slide background-image="./figures/slide-chatpgtinternals.png"}

## Byte-Pair Encoding Tokenizer
- Easy algorithm to *compress* text into most common elements
- [Flying, Trying, Sky, Cry, Sly] --> "F, L, Y, I, N, G, T, R, K, C, S"
- "F, L, Y, I, N, G, T, R, K, C, S, **IN**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, **LY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, **RY**"
- "F, L, Y, I, N, G, T, R, K, C, S, IN, LY, RY, **ING**"
- etc...

## Byte-Pair Encoding Tokenizer
- Able to:
    - encode you whole vocabulary per definition
    - Chose precize size you want for your model
    - Assign tokens to most *important* parts of vocabulary
- This does mean that English gets more tokens than Dutch:

:::{.columns}
:::{.column width="50%"}
![](./figures/tokenizer-dutch.png)
:::

:::{.column width="50%"}
![](./figures/tokenizer-english.png)

:::
:::

- [Try it Yourself](https://platform.openai.com/tokenizer)

## One-hot Encoding {.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
1\\
0\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
1\\
0\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
1\\
0
\end{pmatrix} & \begin{pmatrix}
0\\
0\\
0\\
1
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Sparse vector of the vocabulary dimension
- 3 out of 4 numbers are uninformative
- 'Expensive' for large corpus of text
- Can we do better?
:::

## Word embeddings{.smaller}

:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Dense vectors of Dimension N (hyperparameter of model ~ 728)
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- Meaningfull representation
- Encoded semantic information: 
:::
::::{.inline-fragment style="margin-left: 25%; text-align: center"}
:::{.fragment} 
King - Man + Woman = 
:::
:::{.fragment} 
Queen 
:::
::::
:::{.incremental}
- These embeddings marked the start of the new NLP era ^[*2013 - Efficient Estimation of Word Representations in
Vector Space: [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)*] 
- Generated using shallow networks to:
    1. Predict middle word from context
    2. Predict context from middle word
- Tuning via Backpropegation and gradient descent
- This still has some issues...
:::

## Word embeddings{.smaller}

$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.85
\end{pmatrix} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \color{red}{\begin{pmatrix}
\phantom{-}0.94\\
\phantom{-}0.79\\
-0.34\\
\phantom{-}0.35
\end{pmatrix}} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$

- Dense vectors
- Latent embedding ![](./figures/word-embeddings.png){.absolute left="65%" top="55%" width="auto" height="400px"}
- The numbers are now informative to qualities of the token
- Semanticly-meaningfull: 

:::{.fragment style="margin-left: 25%; text-align: center; display: flex"}
King - Man + Woman = Queen 
:::
 
- This still causes problems...

## Transformer Embeddings {.smaller}
:::{data-id="words"}
$$
\begin{array}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
\text{A} & \text{dry} & \text{well!} & \text{Well} & \text{done!} \\
\begin{pmatrix}
\phantom{-}0.33\\
-0.51\\
\phantom{-}0.83\\
\phantom{-}0.12
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.97\\
-0.15\\
-0.11\\
\phantom{-}0.75
\end{pmatrix} & \begin{pmatrix}
\phantom{-}0.54\\
-0.79\\
-0.34\\
\phantom{-}0.22
\end{pmatrix} & \begin{pmatrix}
-0.41\\
\phantom{-}0.79\\
\phantom{-}0.17\\
\phantom{-}0.84
\end{pmatrix} & \begin{pmatrix}
-0.02\\
\phantom{-}0.69\\
\phantom{-}0.54\\
-0.12
\end{pmatrix}
\end{array}
$$
:::
:::{.incremental}
- Word-embedding now depends on context
- Able to encode even more meaningfull information
- Emperically this just works!
- The start of the new age of NLP ^[*2017 - Attention is all you need [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)*]
:::

## What does self-attention look like {auto-animate="true"}
![](figures/fully_connected.png){fig-align="center"}

## What does self-attention look like {auto-animate="true"}

![](figures/fully_connected.png){.absolute left="70%" top="70%" width="auto" height="250px"}
$$
Attention \sim Query \cdot Key^{T} 
$$

:::{.incremental}
- Conceptual Interpretation:
    - Query: I have a *Noun*, I need a Subject!
    - Key: I have a *Subject* here. 
:::

## What does self-attention look like 

![](figures/fully_connected.png){.absolute left="70%" top="70%" width="auto" height="250px"}
$$
\mathrm{Output\ embedding} \sim \mathrm{Softmax}( Query \cdot Key^{T}) Value
$$

:::{.nonincremental}
- Conceptual Interpretation:
    - Query: I have a *Noun*, I need a Subject!
    - Key: I have a *Subject* here. 
:::
- Data-dependent aggregation of information



## What does self-attention look like

<iframe width="80%" height="80%" justify-content=center src="./figures/html_head.html" style="display: block; margin: 0 auto;"></iframe>


::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Are you still following what is happening? {auto-animate="true"}

<iframe width="80%" height="80%" src="./figures/html_transformer.html" scrollable="no"style="display: block; margin: 0 auto;"></iframe>

::: aside
[1. jessevig/bertviz](https://github.com/jessevig/bertviz)
:::

## Sampling output tokens
- Output is sampled, and therefore a stochastic variable:
    - **Running the same prompt twice will give 2 different results**
$$
P_i = \frac{e^{\frac{y_i}{T}}}{\sum_i^n e^{\frac{y_i}{T}}} = \frac{(e^{y_i})^\frac{1}{T}}{\sum_i^n (e^{y_i})^\frac{1}{T}}
$$

:::{.panel-tabset}

## T = 1 (normal)

![](./figures/ptoken-t1.png){fig-aling=center}

## T = 0.5 

![](./figures/ptoken-t05.png){fig-aling=center}


## T = 2

![](./figures/ptoken-t2.png){fig-aling=center}


## T = 0

![](./figures/ptoken-t0.png){fig-aling=center}

:::

## {.full-bg-slide background-image="./figures/slide-chatpgtinternals.png"}

## Attention and BERT
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- Pretraining on massive corpora of text
- Very powerfull contextual embeddings of language
- *State-of-the-art* on many language tasks with finetuning:
    - Sentiment Analysis
    - Text Classification
    - Named entity recognition
    - Question Answering
    - Language Modeling
- Bert-base: 110m parameters
- Bert-large: 340m parameters

## If we have more compute ...
- What if we want to improve our models.
- Companies like OpenAI have more compute available, what should they do?

:::{.columns}

:::{.column width="50%"}
![](./figures/openai-compute-curves.png)
Optimal model size grows smoothly with the loss target and compute $\mathrm{budget^1}$
:::

:::{.column width="50%"}
![](./figures/openai-compute-allocation.png)
For optimally compute-efficient training, most of the increase should go towards increased model
size^[[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)]
:::
:::

## ... we puth it towards the model scale
![Alt text](./figures/model_size_growing.jpg)
Models just kept on growing, credit: [Julien Simon, Huggingface](https://huggingface.co/blog/large-language-models)

## GPT3 *IS* ChatGPT, almost ... {.smaller}

:::{.columns}

:::{.column width="50%"}

- [2020 - Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- These models are so good at language modeling (*SOTA*),\
 that finetuning is no longer needed to perform NLP task
- Very strong 0-shot performance in many NLP task (already in GPT2)
- Can perform <span class ="highlighted-text" >**In-context learning**</span>:
    - Given a few examples, learn how to perform task
    - *No model parameters are adjusted at any point*
    - <span class="highlighted-text"> Emergent property </span> of LLMs
:::

:::{.column width="50%"}
![](./figures/few-shot-learning.webp)
:::

:::

## 
![[Andrej Karpathy, Microsoft - State of GPT](https://karpathy.ai/stateofgpt.pdf)](./figures/stateofgpt.png)

## Instruction tuning (finetuning)

- The models just predict likely continuations of text.
- This does not match with the behaviour we seek (alignment)

:::{.panel-tabset}
## Base model
```
INPUT:
Explain what a Large Language Model is.

OUTPUT:
Explain what a transformer model is.

Explain what a tokenizer is.

Explain what gradient descent is.
```

## Instruction tuning 
```
INPUT:
Explain what a Large Language Model is.

HUMAN EXAMPLE:
A Large Language Model is a foundational language model with typically billions of parameters. These models have become popular in recent years because of their ease of use combined with impressive performance across the board on NLP-tasks
```
:::

## Reinforcement Learning From Human Feedback

- Not just showing example completions, but also rating them.
- Apply Reward Modeling and Reinforcement learning to 
tune the model towards higher quality responses.
- Very difficult to train, but...
- It just works better.

## And this gives you...

<iframe src="./webpages/Hello world!.html" class="iframe-chatgpt"></iframe>

## Overview 
::: {.nonincremental}
- <span class="highlighted-text">A brief history of LLMs</span>
- Capabilities of ChatGPT
:::

## Overview 
::: {.nonincremental}
- A brief history of LLMs
- <span class="highlighted-text">Capabilities of ChatGPT</span>
:::
# Capabilities of ChatGPT

## Strenghts of LLMs

Foundational language model that:

- <span class ="highlighted-text" >*'Understands'*</span> language conventions (syntax, grammer, etc.)
- Can answer questions (has internal knowledge base)
- Can code
- Can write just like humans
- Can do some basic arithmatic
- Can understand *deep* elements of writing: sentiment, style, etc.
- Can do logical reasoning (to some extent)
- [Capabilities measured on easy scales](https://openai.com/research/gpt-4)

## Weaknesses of LLMs

- Limited <span class ="highlighted-text" >context window</span> (*input size*) 
- It has <span class ="highlighted-text" >no memory</span>
- It can only predict next tokens, nothing else:
    - It cannot perform tasks while you wait!
- It cannot `think' in `multiple directions' like we do!
- `Thinking' only has a fast mode, no thinking fast and slow.
- It doesn't understand it's own internal model!
- It <span class ="highlighted-text" >doesn't know what the truth is</span> and what it is not
- It doesn't understand tabular data 
- It doesn't proces language in a human way ([The Reversal Curse](https://arxiv.org/abs/2309.12288))

:::{.notes}
"Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. 
:::

## Common mistakes
::: {.callout-caution}

## Be Carefull

-	<span class="highlighted-text">LLMs don't learn continuously!</span> *LLMs have a seperate training phase, they need frequent updates for recent events*
-   <span class="highlighted-text">**LLMs don't work like our brains!**</span> *On a conceptual level biological neurons inspired the technique, but in practice they worlds apart in how they function and why*
-   <span class="highlighted-text">**LLMs don't search in a database**</span> *LLMs learn a statistical distribution, and use this to predict the best next token.[^1]*  

:::

[^1]: Unless results from a search engine are explicitly used as an input (Copilot in Bing search, Gemini in Google Search, search on ChatGPT.com, etc.)

## How you should not think of ChatGPT

- We tend to feel empathy for ChatGPT
    - People like to say thank you to ChatGPT
    - It feels very <span class="highlighted-text">human</span> to interact with
- BUT: it really boils down to a very very big autocomplete next-word predictor
- WE CAN BREAK THE MODEL LOGIC

## Repeated sampling Penalty
- Generative language models have a tendency to repeat themselves:
    - Therefore the sampling algorithm receives a <span class="highlighted-text">repetition penalty</span>
    - This can be exploited

<iframe src="./webpages/breaking the algorithm.html" class="iframe-chatgpt" style="width: 700px; height: 900px;" ></iframe>

## Glitch Tokens
- Some tokens identified by BPE only appear in <span class="highlighted-text">useless</span> context
    - Such as /r/counting
    - usernames get very high frequency and get their own tokens

:::{.columns}

:::{.column width="40%"}

<iframe src="./webpages/Glitchtokens example behaviour.html" class="iframe-chatgpt"></iframe>

:::

:::{.column width="60%"}

<iframe width="768px" height="480px" src="https://www.youtube.com/embed/WO2X3oZEJOA?si=Cz6-j5f_Yvr4zvyC&amp;clip=UgkxQwEYjoJKAiZBm9KEcOIswK1XsixP9nJ0&amp;clipt=EAAY758C" frameborder="12px" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture;" allowfullscreen></iframe>

:::

:::

# ChatGPT3.5 vs GPT4{.smaller}

| Feature/Aspect           | GPT-3.5                                      | GPT-4(o)(o)                                               |
|--------------------------|----------------------------------------------|-----------------------------------------------------|
| **Problem-Solving**      | Standard problem-solving skills                 | Greater accuracy and broader knowledge              |
| **Creativity**           | Standard                                     | More creative and collaborative                      |
| **Reasoning Capabilities**| Standard reasoning                          | Advanced reasoning capabilities                     |
| **Context Length**       | 8K tokens context                  | 32K to 128K tokens context                             |
| **Multimodality**        | None                                         | - Vision <br> - Speech in  <br> - Speech out <br> - Image generation    |
| **Extra features**   | None                                    | - RAG <br> - Advanced Data Analysis  |
| **Factual Accuracy**     | Standard                   | more likely to produce factual responses        |
| **Command Adherence**    | Standard                         | More likely to adhere to prompt                     |
| **Availability**         | Free for everybody                       | Free for everybody, higher rate limits for paying customers                                |
: {.custom-table }