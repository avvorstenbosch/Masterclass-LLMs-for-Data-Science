---
title: "Ethical Considerations in Using LLMs"
subtitle: "*The good, the Bad, and the Ugly*"
author: "Alex van Vorstenbosch"
footer: "Ethics and LLMs"
title-slide-attributes:
  data-background-image: "./figures/llms-and-ethics.webp"
  data-background-opacity: "0.5"
resources: 
  - ../webpages/hallucination1_files/*
  - ../webpages/hallucination2_files/*


---

## Overview

\__Disclaimer__\
I don't claim to have the answers.\
Be aware of these topics.\
Form your own opinions and openly discuss issues.

## Overview
:::{.nonincremental}
- <span class="highlighted-text">Biases and Misinformation</span>
- The Dark Side of LLMs
- Group discussion
:::

# Biases and Misinformation

## Biases
- LLMs can strengthen negative stereotypes.
- LLMs can strengten the views of users via confirmation bias:
  - LLMs have a tendency to agree with the user
  - People have a tendency to think that models are *'objective'* and speak the *'truth'*
- LLMs are *'skewed'* to the trainingset majority:
  - English Western views for example 

:::{.fragment}

:::{.smaller}

[Reuters: Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)

:::

![](./figures/reuters-amazon-bias.png){fig-aling=center}
:::

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.
  - Might also be answers that are not supported by the provided context
  - Can be hard to spot as the model is a great *'bluffer'*
    - Doesn't know that the information is wrong
    - asking for self-reflection does help

![](./figures/chatgpt-lawyer.png)

## Hallucinating false information
- What are <span class="highlighted-text">Hallucinations</span>? 
  - It's when LLMs generates incorrect, nonsensical, or unverifiable information presented as fact.

::: {.columns}

::: {.column width="50%"}
 <iframe src="../webpages/hallucination1.html" class="iframe-chatgpt"></iframe>

:::

::: {.column width="50%"}
 <iframe src="../webpages/hallucination2.html" class="iframe-chatgpt"></iframe>

:::

:::

## Who is responsible when AI makes a mistake?

- [First recorded death of the driver of a self-driving car in 2016](https://en.wikipedia.org/wiki/History_of_self-driving_cars#:~:text=The%20first%20known%20fatal%20accident,18%2Dwheel%20tractor%2Dtrailer.)
-  [First recorded fatility of a pedestrian by a self-driving car in 2018](https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg)

![](./figures/self-driving.webp){fig-align="center"}

:::{.notes}
The first known fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The driver was killed in a crash with a large 18-wheel tractor-trailer.

First pedestrian was pushing a bike with shoping bags accross the road.
:::

## Stories of dangerous behaviour by AI chatbots

- [NYtimes: A Conversation With Bing’s Chatbot Left Me Deeply Unsettled](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
  - *"It (red: Sydney chatbot) then tried to convince me that I was unhappy in my marriage, and that I should leave my wife and be with it instead.”*
- *Unable to verify:* [La Libre Belgique story: Without these conversations with the Eliza chatbot, my husband would still be here](https://www.lalibre.be/belgique/societe/2023/03/28/sans-ces-conversations-avec-le-chatbot-eliza-mon-mari-serait-toujours-la-LVSLWPC5WRDX7J2RCHNWPDST24/)


![Screenshots of Business Insider's disturbing conversation with "Eliza," a chatbot from Chai Research.](./figures/dangerous-chatbots.png){fig-align="center"}

## Stories of dangerous behaviour by AI chatbots

- [Google Gemini tells user to 'Please die'](https://gemini.google.com/share/6d141b742a13)

- [Mother says AI chatbot led her son to kill himself in lawsuit against its maker](https://www.theguardian.com/technology/2024/oct/23/character-ai-chatbot-sewell-setzer-death)

## Importance of human-in-the-loop
- **Quality Control:** Because of the generative design LLMs are prone to errors. Require human checks and feedback to ensure accuracy of the output.
- **Human (ethical) Judgement:** Some decisions require human (ethical) judgment, especially in complex, nuanced situations where the context matters.

<span class="highlighted-text"> My personal beliefs: </span>

  - Generative AI is an amazing transformative tool, but not an autonomous agent
  - You are responsible for the mistakes you make when using generative AI:
    - *Make sure the risks are known*
    - *Make sure the risks are manageble*
    - *If not possible, make sure the risks are acceptable*


## What constitutes appropriate content? {.smaller}

| Model                | Source        | Restrictions                              |
|----------------------|---------------|-------------------------------------------|
| ChatGPT by OpenAI    | Closed source | Strongly moderated and curated            |
| Grok by Xai          | Closed source | Less restrictions                         |
| Llama-models by Meta | Open source   | Can be finetuned for any purpose          |

:::{.fragment}
> **Keep in mind**: Nobody is sharing the most important part: <span class="highlighted-text">HIGH QUALITY DATA</span>
:::


:::{.fragment}
![*A reddit pol on the most annoying ChatGPT responses*](./figures/users_getting_tired_of_responses.jpg){height=500px}
:::

## Overview
:::{.nonincremental}
- <span class="highlighted-text">Biases and Misinformation</span>
- The Dark Side of LLMs
- Group discussion
:::

## Overview
:::{.nonincremental}
- Biases and Misinformation
- <span class="highlighted-text">The Dark Side of LLMs</span>
- Group discussion
:::

# The Dark Side of LLMs

## Copyright issues and LLMs
- Can the output be copyrighted?
- Can they be trained on copyrighted materials?
- Can you claim contain generated by LLMs is infringing copyright?
- Open source code does not mean no licenses apply:
  - GPL: *"You may use my code as long as you also release your code open source"*

:::{.columns}

:::{.column width="10%"}
:::

:::{.column width="35%"}
<blockquote class="twitter-tweet"><p  lang="en"><a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a> copilot, with &quot;public code&quot; blocked, emits large chunks of my copyrighted code, with no attribution, no LGPL license. For example, the simple prompt &quot;sparse matrix transpose, cs_&quot; produces my cs_transpose in CSparse. My code on left, github on right. Not OK. <a href="https://t.co/sqpOThi8nf">pic.twitter.com/sqpOThi8nf</a></p>&mdash; Tim Davis (@DocSparse) <a href="https://twitter.com/DocSparse/status/1581461734665367554?ref_src=twsrc%5Etfw">October 16, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



:::

:::{.column width="5%"}
:::


:::{.column width="40%"}
![](./figures/copyright-shield.webp){fig-aling=center}
:::

:::{.column width="10%"}
:::

:::

:::{.notes}
developer Tim Davis, a professor of computer science and engineering at Texas A&M University
:::

## Privacy issues and LLMs
- Can companies train LLMs on (scraped) private data without consent?
  - What if LLMs memorise private data?
- How can we mitigate inference of private information by LLMs?
  - [https://llm-privacy.org/](https://llm-privacy.org/)
- How can we trust third-parties with our proprietary/private information?

## Current view Dutch-government

- ([Overheidsbrede visie Generatieve AI; 01-01-2024]()):\

::: {.fragment style="text-align: center"}
*Niet-gecontracteerde generatieve AI-toepassingen voldoen over het algemeen niet aantoonbaar aan de
geldende privacy- en auteursrechtelijke wetgeving. Zodoende
is het gebruik hiervan door Rijksorganisaties (of in opdracht
daarvan) niet toegestaan, in die gevallen waarin het risico
bestaat dat wetgeving wordt overtreden, tenzij de aanbieder
en de gebruiker **aantoonbaar** voldoen aan de geldende wet- en
regelgeving.*
:::

## Transparency issues of LLMs
- How can we trust models that are "black boxes"?
  - Especially if aren't even sure what these models look like or how they were trained?
- How can these models be used if they can generate 'hallucinations' at any point?
- How can we prevent the use of LLMs for unsuited usecases?

## Misuse of LLMs
:::{.columns}
:::{.column width="60%"}
- How can we prevent the automated generation of misinformation at scale?
- How can we prevent the use of these techniques for spam, identity fraud, and worse?
- Who should decide what misuse of LLMs means?
![](./figures/spamspamspam.jpg){fig-aling=center}
:::
:::{.column width="40%"}
![](./figures/facebook_fake_pages.jpeg){fig-aling=center height=800px}
:::
:::

## Are these AI developments safe?
 
:::{.columns}

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65452940/vpid/p0fkvt9j"></iframe>

:::

:::{.column width="33%"}

<iframe width="500px" height="400" frameborder="0" src="https://www.bbc.com/news/av-embeds/65760449/vpid/p0frhhp3"></iframe>

:::

:::{.column width="33%"}

<iframe width="500px" height="355" frameborder="0" src="https://www.youtube.com/embed/5t1vTLU7s40?si=Sw13s70TPQQ0XWtR&amp;start=7728" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

::: 
:::

:::{.columns}
:::{.column width="33%"}

[BBC: AI 'godfather' Geoffrey Hinton tells the BBC of AI dangers after he quits Google](https://www.bbc.com/news/av/world-us-canada-65453192)

:::

:::{.column width="33%"}
[BBC: AI 'godfather' Yoshua Bengio feels 'lost' over life's work](https://www.bbc.com/news/technology-65760449)
:::

:::{.column width="33%"}
[BBC: Yann LeCun says AI won't take over the world or destroy jobs forever](https://www.bbc.com/news/technology-65886125)
::: 

:::

:::{.notes}
Geoffrey E. Hinton is internationally distinguished for his work on artificial neural nets, especially how they can be designed to learn without the aid of a human teacher. This may well be the start of autonomous intelligent brain-like machines. 

Geoffrey Hinton, Yann LeCun, and Joshua Bengio won the 2018 Turing Award. Highest distinction in computer science.  
:::

## Are these AI developments safe?
:::{.columns}

:::{.column width="50%"}

<div style="height:600px">

![](./figures/yann-lecun-markets.png){width=650px}

</div>

[BI: AI one-percenters seizing power forever is the real doomsday scenario, warns AI godfather](https://www.businessinsider.com/sam-altman-and-demis-hassabis-just-want-to-control-ai-2023-10?international=true&r=US&IR=T)
::: 

:::{.column width="50%"}
- Very real threat in current social media ecosystem:
  - [Romanian election result annulment due to Election Interference](https://www.ifes.org/publications/romanian-2024-election-annulment-addressing-emerging-threats-electoral-integrity)
  - [Report On The Investigation Into Russian Interference In The 2016 Presidential Election, R. Mueller](https://www.justice.gov/archives/sco/file/1373816/dl?inline=)
  - [*Speculative:* Whistleblower: X's Role in 2024 Election Interference via AI-bots](https://theconcernedbird.substack.com/p/elon-musks-and-xs-role-in-2024-election)
:::

::: 

## Are these AI developments safe?


<div style="height:600px">

![](./figures/andrew-ng-markets.png){width=500px}

</div>

[BI: Google Brain cofounder says Big Tech companies are inflating fears about the risks of AI wiping out humanity because they want to dominate the market](https://www.bbc.com/news/technology-65886125)


:::{.notes}
Andrew NG: head of Google Brain and was the former Chief Scientist at Baidu, Stanford professor
:::

## Should there be governmental oversight on AI {.smaller}
- [22 march 2023 - Call for pause on giant AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
  - 6 months
  - develop and implement a shared safety protocols
  - work with policymakers to dramatically accelerate development of robust AI governance systems
  - *"...new and capable regulatory authorities dedicated to AI"*
- [U.S. senate hearing: Oversight of A.I.: Rules for Artificial Intelligence](https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence)

<iframe width="800" height="500" style="display: block; margin-left: auto; margin-right: auto;" src="https://www.youtube.com/embed/IB0y2jidZIk?si=-JfIWbC0-IwFg6fx&amp;controls=0&amp;start=7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## EU AI Act {.smaller}

:::{.columns}

:::{.column width="60%"}

Passed in 2024, main effects:

- Unacceptable risk AI systems will be banned
  - Real time biometric identification
  - Behavioural manipulation
  - Social scoring systems
- Limited Risk AI needs to be transparant:
  - You must know if you are interacting with AI
  - Companies must disclose if content was generated with AI
  - Chatbots are classified as limited risk
- The setup of a new European AI Office to coordinate compliance, implementation, and enforcement of the AI Act
  - Tasked with oversight of General Purpose AI models across Europe

:::

:::{.column width="40%"}
![](./figures/eu.webp)
:::

:::

## The economic impact of AI
- Will it take (many of) our jobs?
- Will it create jobs?
- Or will it just make us more efficient at our current job?

## Climate impact of large language models
- These models are very compute intensive:
  - In the training process
  - But also during inference!
- Estimated to use <span class="highlighted-text">0.5%-2%</span> of global energy usage in 2026\ ([International Energy Agency - Electricity forecast 2024](https://www.iea.org/reports/electricity-2024/executive-summary))
- How can we justify this (inefficient) use of technology?


![](./figures/microsoft-ai-servers.jpg){fig-aling=center}


## Training the model via RLHF

::: {.columns}

::: {.column width="50%"}

- Low-wage workers in Kenia were paid to help collect data for the 'moderation' tool: 
  - Traumatising work
:::

::: {.column width="50%"}
![](./figures/times-sama-2dollar.png)
[Time: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic]("https://time.com/6247678/openai-chatgpt-kenya-workers/")
:::

:::

## Use of LLMs for essays, homework, etc. cannot be reliably detected.
::: {.columns}

::: {.column width="50%"}
- AI-detectors don't work, which is creating serious issues for students.
![](./figures/turnitinGPTconstitution.jpg)
:::

::: {.column width="50%"}
- AI-detectors don't work, which is disrupting how homework is given and made.
![](./figures/howtogetdetectedbyturnitin.webp)
:::

:::

## Overview
:::{.nonincremental}
- Biases and Misinformation
- <span class="highlighted-text">The Dark Side of LLMs</span>
- Group discussion
:::

## Overview
:::{.nonincremental}
- Biases and Misinformation
- The Dark Side of LLMs
- <span class="highlighted-text">Group discussion</span>
:::

# Group discussion

1. Can LLMs be used if they are trained on copyrighted and/or AVG-protected data?


2. Should LLM usage be constrained by ethical guidelines and content filters?


3. Can LLMs be trusted if hallucinations are an inherent part of these systems? 

:::{.fragment style="font-size:30px"}
Discuss within your group for <span class="highlighted-text"> 5-10 minutes </span>, and then we will discuss the results in a plenary session.
:::
