---
title: "Next steps for LLMs" 
subtitle: "Open source, AutoGPT, Multimodel, and more..."
author: "Alex van Vorstenbosch"
footer: "Next steps for LLMs"
title-slide-attributes:
    data-background-image: "./figures/an-uncertain-future.webp"
    data-background-opacity: "0.5"
---

# medium-large language models

## With Large models come large bills

:::{.columns}
:::{.column width="60%"}
- [Big Tech Struggles to Turn AI Hype Into Profits](https://www.wsj.com/tech/ai/ais-costly-buildup-could-make-early-products-a-hard-sell-bdd29b9f)
- Microsoft reportedly makes a loss of 20 dollars per month on average, for every Github Copilot user with a 10 dollar subscription.
- Numbers for OpenAI might be similair for ChatGPT+ users.
- Running LLMs is a very costly endeavour
:::

:::{.column width="40%"}
![](./figures/burning-money.webp)
:::

:::

## A case for small large language models
- Retracted [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) possible leaked size of ChatGPT-3.5-Turbo:
    - 20B parameters
    - Deemed likely by experts due to model response speed
    - Open source models at 13B appear to be somewhat comparable 
- OpenAI appears to be shifting towards smaller faster models

## A case for small large language models
- [Deepmind 2022: Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)

![](./figures/isoloss-contours-deepmind.png){fig-align="center"}

- Gopher is a 280B parameters model.
- The road to improvement has become more data, not more parameters.
- Data may now be the bottleneck, even given the incredibly large datasets.
- Most papers are rather vague on their data collection...

## A case for small large language models

- Smaller language models:
    - Are faster
    - Less expensive
- Wouldn't require GPU-clusters. 
- Could lead to more personalised AI in the long run...
- ... everybody has their own small-LLM as a personal assistant

## A case for small large language models
- Retracted [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) possible leaked size of ChatGPT-3.5-Turbo:
    - 20B parameters is still 40GB of VRAM:
        - Most companies don't have the hardware to run these models
        - Consumers especially don't have the hardware to run this
    - You want to get them even smaller 
- The open source community already figured out how to do just that.

# open source

## A case for open source

- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) 
- open source is *‚Äúfaster, more customizable, more private, and pound-for-pound more capable.‚Äù*
- Researcher claims open source will outpace Google, Openai
- *"the one clear winner in all of this is Meta"*

![](./figures/we-have-no-moat.webp)

## A case for open source

- Open source developments on LLama Models as released by Meta:
    - LORA finetuning of models on consumer laptop.
    - Quantisation makes LLama run at usable tokens/second on consumer laptop.
    - [Achieved parity with 5 million dollar models with 600 dollars](https://crfm.stanford.edu/2023/03/13/alpaca.html)

## Why is open source interesting for you?

- <span class="highlighted-text"> No third party </span> = No need to share private/proprietary data 
- <span class="highlighted-text"> Cheaper to use </span>
- Full control to make your own domain specific model (*hard*)
- You can add your own improvements in the open source ecosystem

## Why open source might be less interesting for you

- <span class="highlighted-text"> Require specialised hardware </span> for best performance
- <span class="highlighted-text"> Not user friendly </span> compared to paid models
- <span class="highlighted-text"> Not SOTA </span> (even though it comes close)

## Currently open source LLMs rely on Meta LLama models a lot

![[https://ai.meta.com/llama/](https://ai.meta.com/llama/)](./figures/llama3.png)

Most models are released in half-precision (16 bit):

- 7B parameters ~ 14 GB of (V)RAM
- 13B parameters ~ 26 GB of (V)RAM
- 70B parameters ~ 140 GB of (V)RAM 
how to run these huge models?

## Try out huggingchat

![](./figures/huggingface.png){.absolute left="20%" width="auto" height="500px"}

<div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%)">
  <span class="highlighted-text" style="font-size: 150px;">[Demo](https://huggingface.co/chat/)</span>
</div>

## AutoGPT - An Autonomous GPT-4 Experiment

[AutoGPT github](https://github.com/Significant-Gravitas/AutoGPT):

:::{.columns}

:::{.column width="60%"}
- üåê Internet access for searches and information gathering
- üíæ Long-term and short-term memory management
- üß† GPT-4 instances for text generation
- üîó Access to popular websites and platforms
- üóÉÔ∏è File storage and summarization with GPT-3.5
- üîå Extensibility with Plugins
:::

:::{.column width="40%"}
![](./figures/autogpt-stars.svg)
:::

:::

## {background-video="./figures/autogpt-code-improvement.mp4"}

## How to find the best model?

![[Berkeley: Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)](./figures/Chatbot-Arena.png){fig-align="center"}

# Next Steps

## Check out the amazing content made by Andrej Karpathy 

<iframe width="1000" height="670" style="display: block; margin-left: auto; margin-right: auto;" src="https://www.youtube.com/embed/zjkBMFhNj_g?si=swKJFOEQfXpXAuA0&amp;start=2100" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Summary
<iframe width="667" height="457" style="display: block; margin-left: auto; margin-right: auto;" src="https://www.youtube.com/embed/zjkBMFhNj_g?si=swKJFOEQfXpXAuA0&amp;start=2100" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
- <span class="highlighted-text"> **System 2 thinking** </span>
- <span class="highlighted-text"> **Self supervising** </span>
- <span class="highlighted-text"> **LLM OS** </span>

## LLM OS - Multimodal models {.full-bg-slide background-image="./figures/multimodal.png"} 

## Multimodal models - Vision

![](./figures/multimodal_vision.png){fig-align="center"}

## LLM OS - Live demo

<iframe width="1000" height="670" style="display: block; margin-left: auto; margin-right: auto;" src="https://www.youtube-nocookie.com/embed/mzdvw_euKlk?si=ZuEWAcZFjhc_xLGI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## LLM OS - Microsoft Copilot

![](./figures/microsoft-copilot.avif)

## Reinforcement Learning - Self-Improvement

:::{.columns}

:::{.column width="60%"}
- Currently, LLMs models are only learning language true human feedback (imitation)
- One of the big innovations in Reinforcement Learning is <span class="highlighted-text">Self-Improvement</span>:
    - For example playing games against yourself to keep self improving
- Unclear how to achieve this for Language
- If possible for domain specific tasks, could propel super-human capibilities of LLMs.^[[Andrej Karpathy - Self-improvement of LLMs](https://www.youtube.com/watch?v=zjkBMFhNj_g&ab_channel=AndrejKarpathy)]
:::

:::{.column width="40%"}
![](./figures/alpha-go.jpg)
:::

:::

## Reinfocement Learning - Breaking news!

[New OpenAI models: Reasoning with LLMs](https://openai.com/index/learning-to-reason-with-llms/)


## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp"} 

## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp" background-opacity=0.3} 
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)

> *"The central claim of our work is that GPT-4 attains a form of general intelligence, indeed showing sparks of artificial general intelligence. This is demonstrated by its core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform (e.g., playing games, using tools, explaining itself, ...). A lot remains to be done to create a system that could qualify as a complete AGI. We conclude this paper by discussing several immediate next steps, regarding defining AGI itself, building some of missing components in LLMs for AGI, as well as gaining better understanding into the origin of the intelligence displayed by the recent LLMs."*

- Overblown claim or a vision for the future to come?
- Let's first find a definition AGI...