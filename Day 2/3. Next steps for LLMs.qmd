---
title: "Next steps for LLMs" 
subtitle: "RAG, open source, AutoGPT, and more..."
author: "Alex van Vorstenbosch"
footer: "Next steps for LLMs"
title-slide-attributes:
    data-background-image: "./figures/an-uncertain-future.webp"
    data-background-opacity: "0.5"
date: "11-24-2023"
---

# medium-large language models

## With Large models come large bills
- [Big Tech Struggles to Turn AI Hype Into Profits](https://www.wsj.com/tech/ai/ais-costly-buildup-could-make-early-products-a-hard-sell-bdd29b9f)
- Microsoft reportedly makes a loss of 20 dollars per month on average, for every Github Copilot user with a 10 dollar subscription.
- Numbers for OpenAI might be similair for ChatGPT+ users.
- Running LLMs is a very costly endeavour

## A case for small large language models
- Retracted [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) possible leaked size of ChatGPT-3.5-Turbo:
    - 20B parameters
    - Deemed likely by experts due to model response speed
    - Open source models at 13b appear to be somewhat comparable 
- OpenAI appears to be shifting towards smaller faster models

## A case for small large language models
- [Deepmind 2022: Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)

![](./figures/isoloss-contours-deepmind.png){fig-align="center"}

- The road to improvement has become more data, not more parameters.
- Data may now be the bottleneck, even given the incredibly large datasets.
- Most papers are rather vague on their data collection...

## A case for small large language models

- Smaller language models:
    - Are faster
    - Less expensive
- Wouldn't require gpu-clusters 
- Could lead to more personalised AI in the long run...
- ... Everybody has their own small-LLM as a personal assistant

## A case for small large language models
- Retracted [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) possible leaked size of ChatGPT-3.5-Turbo:
    - 20B parameters is still 40GB of VRAM:
        - Most companies don't have the hardware to run these models
        - Consumers especially don't have the hardware to run this
    - You want to get them even smaller 
- The open source community already figured out how to do just that.

# open source

## A case for open source

- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) 
- open source is ‚Äúfaster, more customizable, more private, and pound-for-pound more capable.‚Äù
- Researcher claims open source will outpace Google, Openai
- "the one clear winner in all of this is Meta"
![](./figures/we-have-no-moat.webp)

## A case for open source

- Open source developments on LLama Models as released by Meta:
    - LORA finetuning of models on consumer laptop.
    - Quantisation makes LLama run at usable tokens/second on consumer laptop.
    - [Achieved parity with 5 million dollar models with 600 dollars](https://crfm.stanford.edu/2023/03/13/alpaca.html)

## Why is open source interesting for you?

- <span class="highlighted-text"> No third party = No need to share private/proprietary data </span>
- Cheaper to use
- Full control to make your own domain specific model (hard)
- You can add your own improvements in the open source ecosystem

## Why open source might be less interesting for you

- Does require specialised (expensive) hardware for best performance
- Depending on usecase, need to deploy the model somewhere
- No quick and easy API anymore

## Currently open source LLMs rely on meta LLama2 model

![[https://ai.meta.com/llama/](https://ai.meta.com/llama/)](./figures/Llama2.png)

Most models are released in half-precision (16 bit):

- 7B parameters ~ 14 GB of (V)RAM
- 13B parameters ~ 26 GB of (V)RAM
- 70B parameters ~ 140 GB of (V)RAM 
- How to run these huge models?

## Quantization to the rescue

- Smart algorithms to reduce the weights to 8-bit, 4-bit format with minimal reduction in model performance.
    - 7B parameters at 4 bit ~ 3.4 GB of (V)RAM
    - 13B parameters ~ 7.8 GB of (V)RAM
    - 70B parameters ~ 42 GB of (V)RAM 

## Quantization rules of thumb

- For best performance:
    - Biggest model with biggest quantization that fits into (GPU-)memory
    - model-size > quantization: 13B at 8bit is better than 7B at full size.
- Balancing performance/speed:
    - 4 bit quantizated models


## AutoGPT - An Autonomous GPT-4 Experiment

[AutoGPT github](https://github.com/Significant-Gravitas/AutoGPT):

:::{.columns}

:::{.column width="60%"}
- üåê Internet access for searches and information gathering
- üíæ Long-term and short-term memory management
- üß† GPT-4 instances for text generation
- üîó Access to popular websites and platforms
- üóÉÔ∏è File storage and summarization with GPT-3.5
- üîå Extensibility with Plugins
:::

:::{.column width="40%"}
![](./figures/autogpt-stars.svg)
:::

:::

## {background-video="./figures/autogpt-code-improvement.mp4"}

## Tips and Tricks AutoGPT
- For long conversations:
    - memory management by:
        - Offloading long term memory to local storage
        - Keeping track of the short term memory via summarization
- Have the model write is own CoT-prompting commands
- Have the model criticise it's own plans (self-reflection)
- Allowing the model to interact with many different API-endpoints (as well as OS)

# Retrieval Augmented Generation

## {.full-bg-slide background-image="./figures/retrieval-augmented-generation.png"}

## {.full-bg-slide background-image="./figures/retrieval-augmented-generation2.png"}

## GPT store

:::{.columns}

:::{.column width="20%"}
- ChatGPT
- \+ API function calling
- \+ RAG
:::

:::{.column width="80%"}
![](./figures/introducing-gpts.avif)
:::

:::

## GPT store

:::{.columns}

:::{.column width="40%"}
- ChatGPT
- \+ API function calling: connect ChatGPT to any external API
- \+ RAG: Add knowledge base to model
- Smart way to get acces to more training data
:::

:::{.column width="60%"}
![](./figures/introducing-gpts.avif)
:::

:::

# What will come next?

## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp"} 

## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp" background-opacity=0.3} 
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)

> *"The central claim of our work is that GPT-4 attains a form of general intelligence, indeed showing sparks of artificial general intelligence. This is demonstrated by its core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform (e.g., playing games, using tools, explaining itself, ...). A lot remains to be done to create a system that could qualify as a complete AGI. We conclude this paper by discussing several immediate next steps, regarding defining AGI itself, building some of missing components in LLMs for AGI, as well as gaining better understanding into the origin of the intelligence displayed by the recent LLMs."*

- Overblown claim or a vision for the future to come?
- Let's first find a definition AGI...