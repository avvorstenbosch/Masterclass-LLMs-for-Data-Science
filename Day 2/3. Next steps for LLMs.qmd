---
title: "Next steps for LLMs" 
subtitle: "RAG, open-source, AutoGPT, and more..."
author: "Alex van Vorstenbosch"
footer: "Next steps for LLMs"
title-slide-attributes:
    data-background-image: "./figures/an-uncertain-future.webp"
    data-background-opacity: "0.5"
date: "11-24-2023"
---

## Retrieval Augmented Generation

## A case for small large language models
- [Big Tech Struggles to Turn AI Hype Into Profits](https://www.wsj.com/tech/ai/ais-costly-buildup-could-make-early-products-a-hard-sell-bdd29b9f)
- Microsoft reportedly makes a loss of 20 dollars per month on average, for every Github Copilot user with a 10 dollar subscription.
- Numbers for OpenAI might be similair for ChatGPT + users.

## A case for small large language models
- Retracted [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) possible leaked size of ChatGPT-3.5-Turbo:
    - 20B parameters
    - Deemed likely by experts due to model response speed
    - Open source models at 13b appear to be somewhat comparable 

## A case for small large language models
- [Deepmind: Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)

![](./figures/isoloss-contours-deepmind.png){fig-align="center"}

- The road to improvement has become more data, not more parameters
- Data may now be the bottleneck, even given the incredibly large datasets.
- Most papers are rather vague on their data collection...

## A case for open source
- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) 
- open source is ‚Äúfaster, more customizable, more private, and pound-for-pound more capable.‚Äù
- Researcher claims open source will outpace Google, Openai
- "the one clear winner in all of this is Meta"
![](./figures/we-have-no-moat.webp)

## A case for open source
- Open source developments on LLama Models
    - LORA finetuning of models on consumer laptop.
    - Quantisation makes LLama run at usable tokens/second on consumer laptop.
    - [Achieved parity with 5 million dollar models with 600 dollars](https://crfm.stanford.edu/2023/03/13/alpaca.html)

## Why is open source interesting for you?
- <span class="highlighted-text"> No third party = No need to share private/proprietary data </span>
- Cheaper to use
- Full control to make your own domain specific model (hard)
- You can add your own improvements in the open source ecosystem

## Why open source might be less interesting for you
- Does require specialised (expensive) hardware for best performance
- Depending on usecase, need to deploy the model somewhere
- No quick and easy API anymore

## Currently open source LLMs rely on meta LLama2 model

![[https://ai.meta.com/llama/](https://ai.meta.com/llama/)](./figures/Llama2.png)

Most models are released in half-precision (16 bit):

- 7B parameters ~ 14 GB of (V)RAM
- 13B parameters ~ 26 GB of (V)RAM
- 70B parameters ~ 140 GB of (V)RAM 
- How to run these huge models?

## Quantization to the rescue
- Smart algorithms to reduce the weights to 8-bit, 4-bit format with minimal reduction in model performance.
    - 7B parameters at 4 bit ~ 3.4 GB of (V)RAM
    - 13B parameters ~ 7.8 GB of (V)RAM
    - 70B parameters ~ 42 GB of (V)RAM 

## Quantization rules of thumb
- For best performance:
    - Biggest model with biggest quantization that fits into (GPU-)memory
    - model-size > quantization: 13B at 8bit is better than 7B at full size.
- Balancing performance/speed:
    - 4 bit quantizated models


## AutoGPT - An Autonomous GPT-4 Experiment

[AutoGPT github](https://github.com/Significant-Gravitas/AutoGPT):

:::{.columns}

:::{.column width="60%"}
- üåê Internet access for searches and information gathering
- üíæ Long-term and short-term memory management
- üß† GPT-4 instances for text generation
- üîó Access to popular websites and platforms
- üóÉÔ∏è File storage and summarization with GPT-3.5
- üîå Extensibility with Plugins
:::

:::{.column width="40%"}
![](./figures/autogpt-stars.svg)
:::

:::

## {background-video="./figures/autogpt-code-improvement.mp4"}

## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp"} 

## Sparks of AGI? {.full-bg-slide background-image="./figures/an-uncertain-future.webp" background-opacity=0.3} 
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)

> *"The central claim of our work is that GPT-4 attains a form of general intelligence, indeed showing sparks of artificial general intelligence. This is demonstrated by its core mental capabilities (such as reasoning, creativity, and deduction), its range of topics on which it has gained expertise (such as literature, medicine, and coding), and the variety of tasks it is able to perform (e.g., playing games, using tools, explaining itself, ...). A lot remains to be done to create a system that could qualify as a complete AGI. We conclude this paper by discussing several immediate next steps, regarding defining AGI itself, building some of missing components in LLMs for AGI, as well as gaining better understanding into the origin of the intelligence displayed by the recent LLMs."*

- Overblown claim or a vision for the future to come?
- Let's first find a definition AGI...