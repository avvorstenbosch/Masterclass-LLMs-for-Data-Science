---
title: "NLP with LLMs" 
date: "11-24-2023"
format: 
    html:
        number-sections: true
        page-layout: full
        title-block-banner: true
        self-contained: true
include-in-header:
  - text: |
      <style>
      .cell-output-stdout code {
        word-break: break-wor !important;
        white-space: pre-wrap !important;
      }
      </style>
---

# Sentiment Analysis

In this Exercise set, we are going to use ChatGPT for a sentiment analysis problem. 

## Environment setup
Load all the libraries we will need, install the libraries if they are not yet installed. 
```{r}
library(tidyverse)
library(caret)
library(kableExtra)
library(knitr)
```

We will also load some utility functions I have written for you so you don't have to:

```{r}
utils_dir <- "./utils/"
for (file in list.files(utils_dir)) {
  source(paste0(utils_dir, file))
}
```

Please check out the Documentation under `./utils/chat_completion.R` to get an overview of what the `generate_completion` function looks like. 

## Read the data
The dataset contains 500 samples of the SemEval2014-restaurants dataset. This is a dataset for ABSA:\
Aspect Based Sentiment Analysis: Detect all relevant aspects in a text along with the sentiment expressed around them. 

There are 2 flavours of ABSA:

- regular ABSA: Detect the sentiment for each aspect given. 
- E2E-ABSA: Detect both the aspects and their sentiment. 

E2E is significantly harder to achieve, and to a higher degree depends on the labeling quality of the dataset. 

```{r}
data_x <- read_csv2("./data/semEval2014-restaurants-x.csv")
data_y <- read_csv2("./data/semEval2014-restaurants-y.csv")

# Not all sentences in the dataset have a y-label, as not all sentences have a sentiment expression in them
# For now, we ignore this. 
data_y <- data_y %>% drop_na()
data <- data_x %>% inner_join(data_y, by="sentence_id")
```
# ABSA

## Zero-shot

Let us start with classifying a single example. 
```{r}
system_prompt <- "
  You are a sentiment labeling expert. You will be provided with a review along with a list of aspects. For each aspect, determine the sentiment polarity. 
  Rules:
  Aspect term polarity: Each aspect term has to be assigned one of the following polarities based on the 
  sentiment that is expressed in the sentence: positive, negative, conflict (both positive and negative sentiment), neutral (neither positive nor negative sentiment)
  
  Format your output as a .json a polarity for each input aspect. Only return this json.
"

user_prompt <- "
  aspects: {target_aspect_terms_combined}
  sentence: {target_sentence}
"
```

```{r}
i <- 1
x_sentence_id <- data_x[[i,1]]
target_sentence <- data_x[[i,2]]

targets <- data_y %>% filter(sentence_id==sentence_id)
target_aspect_terms_combined <- paste(targets$aspect_term, collapse = ", ")
```

```{r}
response <- generate_completion(system_prompt, user_prompt)
```
```{r}
results <- c()
response_classifications <- fromJSON(response$choices$message.content)
for (i in seq_along(response_classifications)){
  result_i = response_classifications[[i]]==targets$polarity[targets$aspect_term==names(response_classifications)[[1]]]
  results = c(results, result_i)
}
```

### Multiple examples
No we'll do a bigger test run. Below I've written some code that puts all the data of the first 25 sentences into a single
dataframe. Finish the get_predicitions function to get predictions for every sentence in data_n
```{r}
#| output: false
n = 200

data_n <- data %>%
  group_by(sentence_id) %>%
  mutate(new_id = cur_group_id()) %>%
  ungroup() %>%
  filter(new_id <= n) %>%
  select(-new_id)

predict_sentiment <- function(target_sentence, target_aspect_terms_combined, system_prompt=system_prompt, user_prompt=user_prompt){
  response <- generate_completion(system_prompt, user_prompt, format = list("type" = "json_object"))
  response_classifications <- fromJSON(response$choices$message.content)
  return(response_classifications)
}

# We're making predictions in a for loop, and appending them to a dataframe
sentence_ids <- unique(data_n$sentence_id)
predictions <- data.frame(sentence_id = integer(), aspect_term=character(), prediction=character())
for (target_sentence_id in unique(data_n$sentence_id)){
  targets <- data_n[data_n$sentence_id==target_sentence_id,]
  target_sentence <- targets[[1, "sentence_text"]]
  
  target_aspect_terms_combined <- paste(targets$aspect_term, collapse = ", ")
  
  prediction <- predict_sentiment(target_sentence, target_aspect_terms_combined, system_prompt, user_prompt)
  for (j in seq_along(prediction)){
    predictions <- rbind(predictions, data.frame(sentence_id=target_sentence_id, aspect_term = names(prediction)[[j]], polarity_prediction = prediction[[j]]))
  }
}
```

Bring them together so we can evaluate the accuracy
```{r}
data_n_p <- data_n %>% full_join(predictions, by=c("sentence_id", "aspect_term"))
```

```{r}
library(caret)
# Create a confusion matrix
conf_matrix <- confusionMatrix(factor(data_n_p$polarity_prediction,levels=c("negative", "neutral", "positive", "conflict")), 
                               factor(data_n_p$polarity, levels=c("negative", "neutral", "positive", "conflict")))

# Extracting metrics
accuracy <- conf_matrix$overall['Accuracy']


# Print the metrics
baseline_accuracy <- max((data_n %>% group_by(polarity) %>% summarize(count=n()))$count)/nrow(data_n)
print(glue("The baseline accuracy (predicting majority class): {round(baseline_accuracy,3)}"))
print(glue("Micro Accuracy = Micro Precision = Micro Recall = Micro F1: {round(accuracy,3)}"))
print(round(conf_matrix$byClass[,c("Precision", "Recall", "Specificity")],3))
```

```{r}
plot_confusion_matrix <- function(conf_matrix){
  # Convert to data frame for plotting
  matrix_df <- as.data.frame(conf_matrix$table)
  matrix_df$Prediction <- factor(matrix_df$Prediction, levels = rev(levels(matrix_df$Prediction)))
  
  # Plot using ggplot2
  plot <- ggplot(data = matrix_df, aes(x = Reference, y = Prediction)) +
      geom_tile(aes(fill = Freq), colour = "white") +
      geom_text(aes(label = sprintf("%0.0f", Freq)), vjust = 1) +
      scale_fill_gradient(low = "white", high = "steelblue") +
      theme_minimal() +
      xlab("True Label") +
      ylab("Predicted Label") +
      ggtitle("Confusion Matrix") + 
      scale_x_discrete(position = "top") 
  
  print(plot)
}
plot_confusion_matrix(conf_matrix)
```
```{r}
data_n_p %>% filter(polarity != polarity_prediction) %>% select(sentence_text, aspect_term, polarity, polarity_prediction) %>% kable() %>% kable_styling()
```
```{r}
data_n_p %>% 
  filter(sentence_id %in% (data_n_p %>% 
                             filter(is.na(polarity)) %>% 
                             pull(sentence_id)
                           )
         ) %>% arrange(sentence_id) %>% kable() %>% kable_styling()
```

The model corrects missspelled words in the current version, and it sometimes splits terms into 2 or more terms where it shoudn't

## Few-shot
Now adjust your system prompt, use difficult examples to demonstrate how labels should be given. 
Be sure that these are not labels you will test on later. 

# E2E-ABSA

```{r}
system_prompt <- "
  Given a review, extract the aspect term(s) and determine their corresponding sentiment polarity.
  
  Rules:
  Aspect terms: Single or multiword terms naming particular aspects of the target entity. For example,
  in “I liked the service and the staff, but not the food”, the aspect terms are “service”,
  “staff” and “food”; in “The hard disk is very noisy” the only aspect term is “hard disk”.
  Aspect term polarity: Each aspect term has to be assigned one of the following polarities based on the 
  sentiment that is expressed in the sentence about it: positive, negative, conflict (both positive and negative sentiment), neutral (neither positive nor negative sentiment)

  Format your output as a .json with single list results, which has all pairs of 'aspect':'polarity' detected in the sentence.
  
  Review: {sentence}
"
```